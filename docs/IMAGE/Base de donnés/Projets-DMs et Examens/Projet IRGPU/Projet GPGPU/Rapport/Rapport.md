---
title: Rapport
---

Antoine HAVARD
Emma CASAGRANDE
Paul HAARDT
Salom√© BERGER
# Description du sujet
Ce projet consiste √† d√©tecter la pr√©sence de mouvement sur un flux vid√©o.
Plus pr√©cis√©ment, les objectifs sont multiples:
- traiter un flux vid√©o en temps r√©el, ou √† d√©faut plus vite que son d√©bit d‚Äôentr√©e
- segmenter l‚Äôimage pour indiquer les zones de mouvement identifi√©es
- indiquer √† l‚Äôaider d‚Äôun signal binaire si le mouvement est important
Pour y parvenir, nous prenons avantage de la parall√©lisation GPU.
En effet, plusieurs algorithmes ont √©t√© utilis√©s pour traiter le flux.
## Les algorithmes
Apr√®s avoir impl√©ment√© l‚Äôalgorithme sugg√©r√© dans l‚Äô√©nonc√©, qui est plus ou moins une moyenne glissante, nous avons explor√© d‚Äôautres pistes.
Nous les d√©taillons ici, avec leur aboutissement, et l‚Äôapproche g√©n√©rale. Les benchmarks des approches impl√©ment√©es sont disponibles plus bas.
### Basic background estimation
Cet algorithme est plut√¥t simple. Pour chaque pixel, il assigne une valeur de r√©f√©rence comme √©tant le fond (not√© background apr√®s), et y compare la valeur du pixel de l‚Äôimage actuelle. Cette comparaison se fait avec une formule de calcul de distance euclidienne entre valeurs RGB:
$dist(Pix_{current}, Pix_{background}) = \sqrt{(R_{Pix_{current}} - R_{Pix_{background}})^2 + (G_{Pix_{current}} - G_{Pix_{background}})^2 + (B_{Pix_{current}} - B_{Pix_{background}})^2}$
Ce type de comparaison en RGB ne tient pas compte de la perception humaine des couleurs, mais pour identifier un changement de couleur d√ª au mouvement, il est largement suffisant.
Au bout de $n$ frames (souvent 100), si la comparaison produit toujours une distance suffisamment √©lev√©e (par rapport √† un seuil fix√©), on consid√®re qu‚Äôil y a mouvement. Sinon, on fait une moyenne entre le pixel de background et le pixel actuel.  
**‚úÖAvantages**:
- Simple √† mettre en ≈ìuvre
- Rapide √† calculer
- R√©siste √† de petites variations de couleur dues au bruit
  
**‚ùåInconv√©nients**:
- Initialiser les pixels de r√©f√©rence depuis la premi√®re image peut introduire des biais (comme des artefacts fant√¥mes des objets en mouvement pr√©sents √† la premi√®re image)
- Choisir le nombre d'images $n$ pour d√©terminer si le background change peut √™tre d√©licat. Si trop √©lev√©, il peut y avoir des "traces" derri√®re les objets en mouvement
- Comme on traite un pixel √† la fois, si quelques pixels bougent c√¥t√© √† c√¥te comme dans le cas de feuilles, ce mouvement sera d√©tect√© alors qu'il n'est pas int√©ressant
  
### Moving Average
Cet algorithme utilise un mod√®le de fond adaptatif bas√© sur une moyenne mobile. Il est similaire au pr√©c√©dent, mais on utilise un coefficient $\alpha$ pour pond√©rer l‚Äôimportance du pixel candidat (celui de l‚Äôimage actuelle) avec le pixel du background. On calcule la nouvelle valeur du pixel du background ainsi:
$BG_{new} =\\\alpha \times Pix_{current} + (1-\alpha) \times BG_{old}$
‚úÖ**Avantages:**
- Relativement simple √† impl√©menter
- Rapide √† calculer
- Comme le pr√©c√©dent mod√®le, il s‚Äôadapte aux changements l√©gers
- Le param√®tre $Œ±$ contr√¥le la vitesse d'adaptation, √©vitant les changements brusques
**‚ùåInconv√©nients:**
- M√™mes probl√®mes que le pr√©c√©dent algorithme, m√™me si on est capable de r√©gler l‚Äôimportance des brusques changements
- Le param√®tre $\alpha$ d√©pend fortement de la vitesse pr√©vue des objets sur le flux
### Temporal Median
Cet algorithme utilise un mod√®le de fond statistique bas√© sur la m√©diane des valeurs historiques. Pour chaque pixel, il maintient un historique des $n$ derni√®res valeurs RGB et utilise la m√©diane comme r√©f√©rence de fond. La d√©tection de mouvement se fait par comparaison de distance euclidienne entre le pixel actuel et sa m√©diane historique.
Le pixel de fond est ni plus ni moins que la m√©diane, calcul√©e comme ci-contre:
$BG_{pixel} = median(\text{History}_{pixel}[0...n])$
‚úÖ**Avantages:**
- gr√¢ce √† la m√©diane, les pics de bruits ont peu d‚Äôimpact
- les artefacts temporaires sont ignor√©s
- on peut r√©gler la taille de l‚Äôhistorique pour d√©cider de la vitesse d‚Äôadaptation aux changements
**‚ùåInconv√©nients:**
- stocker un historique de valeurs par pixels est co√ªteux spatialement
- plus l‚Äôhistorique est grand, plus les changements l√©gitimes prennent du temps √† s‚Äôint√©grer
- calculer la m√©diane est complexe computationnellement, m√™me si on peut faire un tri partiel des donn√©es pour la trouver
## Frame Differencing
Le Frame Differencing consiste simplement √† soustraire l'image actuelle de l'image pr√©c√©dente de la s√©quence. Le code est simple √† mettre en ≈ìuvre:
1. Conversion en niveaux de gris
2. Calcul de la diff√©rence absolue entre les pixels de chaque image
3. L'application d'un seuil d√©terminera si l'intensit√© est consid√©r√©e comme un mouvement ou non
$\text{diff}(\text{Frame}_t, \text{Frame}_{t-1}) = \\|\text{Intensity}_{current} - \text{Intensity}_{previous}|$
**‚úÖ Avantages:**
- Simple √† impl√©menter
- Faible utilisation m√©moire
- Rapide √† calculer
- D√©tection imm√©diate des changements
**‚ùå Inconv√©nients:**
- Ombres dans les images qui peuvent causer de fausses cibles
- Les taches dans l'arri√®re-plan apparaissent dans la diff√©rence d'image m√™me si elles ne sont pas en mouvement (bruit de l'image et vibrations de l'appareil photo)
- N'est pas ind√©pendant de la vitesse de l'objet en mouvement - √©choue pour les objets en mouvement lent car la diff√©rence est presque nulle
- Sensible au bruit, aux ombres et aux changements d'√©clairage
- Pas de mod√®le de fond persistant - objets stationnaires temporaires cr√©ent des artefacts
---
# Nettoyage du masque de mouvement - Morphologie math√©matique
Apr√®s d√©tection du mouvement √† l‚Äôaide des diff√©rents algorithmes (background estimation, moyenne glissante, etc.), le masque de mouvement brut peut contenir beaucoup de bruit : des pixels isol√©s, des zones non connect√©es, ou encore des artefacts dus √† la compression vid√©o ou √† la lumi√®re.
Pour am√©liorer la qualit√© de ce masque, une phase de **nettoyage** est effectu√©e, en deux √©tapes principales : **ouverture morphologique** et **seuillage par hyst√©r√©sis**.
### Seuillage par hyst√©r√©sis
Pour affiner encore le masque et supprimer les zones ambigu√´s, un seuillage d‚Äôhyst√©r√©sis est utilis√©, inspir√© de la m√©thode de Canny.
- On commence par d√©tecter les pixels **fortement activ√©s** (au-dessus d‚Äôun seuil `th_high`), consid√©r√©s comme fiables.
- Ensuite, on propage √† leurs **voisins connect√©s** les pixels mod√©r√©ment activ√©s (entre `th_low` et `th_high`).
- Les pixels en dessous de `th_low` sont ignor√©s.
Cela permet de **connecter des zones disjointes**, tout en **√©liminant le bruit faible non connect√©**.
‚úÖ **Avantages :**
- R√©sultats beaucoup plus propres visuellement
- Bords des objets mieux d√©finis
- Supprime efficacement le bruit r√©siduel
‚ùå **Inconv√©nients :**
- Propagation co√ªteuse si l‚Äôimage contient beaucoup de bords faibles
- Complexit√© $O(N)$ mais avec propagation it√©rative (plusieurs passes)
### Ouverture morphologique
L‚Äôouverture est une op√©ration issue de la morphologie math√©matique. Elle consiste en une **√©rosion suivie d‚Äôune dilatation**, appliqu√©e avec un m√™me √©l√©ment structurant.
- **Erosion** : on prend la **valeur minimale** dans le voisinage du pixel. Cela permet d'√©liminer les petites zones bruit√©es.
- **Dilatation** : on prend ensuite la **valeur maximale** dans le m√™me voisinage, ce qui permet de restaurer les zones pertinentes qui ont surv√©cu √† l‚Äô√©rosion.
Deux versions ont √©t√© cod√©es :
- **Carr√©** : le voisinage est un carr√© centr√© sur le pixel, de c√¥t√© `r`.
- **Disque** : le voisinage est un disque (d√©fini comme tous les pixels √† une distance ‚â§ `r`), plus fid√®le √† la g√©om√©trie des objets r√©els.
Les deux op√©rations sont appliqu√©es sur une image binaire (les pixels sont stock√©s avec le type `uint8`), donc les minimums et maximums sont pris entre 0 et 255.
‚úÖ **Avantages :**
- Supprime les faux positifs isol√©s (bruit)
- Conserve la forme globale des objets d√©tect√©s
- √âvite les contours d√©form√©s
‚ùå **Inconv√©nients :**
- Peut √©liminer les petits objets l√©gitimes
- Plus lent avec un √©l√©ment structurant en disque (calcul du masque de disque)
### Impl√©mentation
1. Application du seuillage par hyst√©r√©sis via une propagation r√©cursive
2. Application de l‚Äôouverture morphologique (disque ou carr√©) sur les valeurs binaires produites √† l‚Äô√©tape pr√©c√©dente
---
# Benchmarks & graphiques

> [!important] Il est important de noter que pour des raisons de coh√©rence des donn√©es, les benchmarks ont √©t√© effectu√©s sur la m√™me machine, donc voici les sp√©cifications. La vid√©o utilis√©e est celle fournie initialement dans le sujet.
> 
> |GPU|CPU et RAM|Vid√©o utilis√©e|
> |---|---|---|
> |NVIDIA GeForce RTX 3070 Mobile|AMD Ryzen 7 6800H|320x240 pixels|
> |8GB|16GB|1700 frames|
> |GDDR6|SSD|60fps|
> |Non overclock√©e|64bits architecture|format AVI|
![[image 108.png|image 108.png]]
Le graphe ci-dessus montre le nombre d‚Äôappels total effectu√© et la dur√©e en millisecondes lors des appels m√©moire `memcopy`. Il s‚Äôagit en effet d‚Äôune source importante de d√©lai lors de la cr√©ation d‚Äôune frame.
On peut constater que **Background Estimator** effectue 2 fois plus d‚Äôappels que les autres algorithmes car il doit stocker le nombre de frames √©coul√©es qui sont diff√©rentes du pixel candidat en plus des valeurs de pixels de background elles-m√™mes.
On remarque que **Frame Differencing** fait 2 moins d‚Äôappels vers **l‚Äôappareil h√¥te** car les valeurs de la frame pr√©c√©dente et actuelle sont envoy√©es vers le GPU, mais celui-ci ne renvoie que la valeur des diff√©rences absolues (voir formule dans les algorithmes), d‚Äôo√π la diff√©rence en nombre d‚Äôappels de **h√¥te vers appareil** et **appareil vers h√¥te**.
On note que l'algorithme **Temporal Median** est significativement plus long que les autres (environ 17 fois plus long). C‚Äôest logique puisque cet algorithme conserve un historique de valeurs par pixel, ce qui augmente la quantit√© de donn√©es √† √©changer entre le **host** et le **device** et inversement.
![[Graph_10.png]]
Ce graphe montre le nombre de frames produit par seconde en moyenne.
On constate encore une fois que **Temporal Median** est moins performante que les autres (presque 30 moins performant). Les framerates sont donc les suivants:
|REALTIME PROCESSING ?|==**BG_EST**==|==**FRAME DIFF**==|==**TEMP MEDIAN**==|==**MOVING AVG**==|
|---|---|---|---|---|
|FPS|**2833**|1848|65|1325|
|120|‚úÖ|‚úÖ|‚ùå|‚úÖ|
|60|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
On constate effectivement que nos algorithmes pourraient traiter du flux vid√©o temps r√©el seulement pour certains framerates, et **Temporal Median** m√©riterait quelques optimisations (d√©taill√©es plus bas).
L‚Äôalgorithme temporal median, bien qu‚Äôint√©ressant th√©oriquement, s‚Äôest r√©vel√© fondamentalement co√ªteux en m√©moire et signifiquativement moins performants que les autres approches test√©es, tant en qualit√© de detection qu‚Äôen vitesse.
C‚Äôest pourquoi nous avons choisi de ne pas investir de temps dans son optimisation au profit d‚Äôalgotithmes plus l√©ger et plus efficaces.
---
# Analyse des bottlenecks et optimisations
L'objectif de cette √©tape est d'examiner les diff√©rents comportements de nos impl√©mentations et de voir o√π se situent les bottlenecks potentiels.
Pour cela, nous utilisons **Nsight Systems** qui fournit un examen au niveau du syst√®me. Nous avons ajout√© l'option `--trace=cuda,nvtx` pour sp√©cifier le suivi pour les ex√©cutions CUDA et NVTX.
Dans l'ensemble, Nsight Systems a fourni une analyse dans les points suivants:
- **Data Transfer Analysis:** transfert de donn√©es entre host (CPU) et device (GPU)
- **Kernel Overview:** analyse des kernels
En particulier, nous avons examin√© le **taux d‚Äôoccupation (occupancy)** de notre impl√©mentation (l'efficacit√© avec laquelle nous utilisons les ressources de calcul du GPU) :
1. **Latence Instructions**: Le temps n√©cessaire √† l'ex√©cution des instructions.
2. **Transfert de donn√©es**: Le temps n√©cessaire pour d√©placer les donn√©es entre host et device.
Dans un premier temps, on a travaille √† faire des impl√©mentations d‚Äôalgorithmes sans mask cleaning pour avoir une id√©e du fonctionnement.
==**Pas d‚Äôoptimisations + Pas de Mask Cleaning**== ‚úÖ
|Dur√©e en ¬µs|==**BG_EST**==|==**MOVING AVG**==|==**TEMP MEDIAN**==|==**FRAME DIFF**==|
|---|---|---|---|---|
|CUDA API  <br>Summary (cuda_api_sum)|1126|**519**|17525|585|
|CUDA GPU Kernel Summary  <br>(cuda_gpu_kern_sum)|21|**20**|11247|28|
|CUDA GPU MemOps Summary (cuda_gpu_mem_time_sum)|227|**122**|4114|142|
|FPS GPU|728|**1513**|30|1325|
Premi√®rement, on remarque que **Temporal Median** pr√©sente des temps d'ex√©cution disproportionnellement √©lev√©s dans les deux configurations. Avec 17525¬µs d'API et 11247¬µs de kernel en configuration non-optimis√©e, cet algorithme limite s√©v√®rement le d√©bit, environ 25-50x plus lent que les autres algorithmes.
**Temporal Median** maintient un ratio √©lev√© (1.56 non-optimis√©, 0.26 optimis√©), r√©v√©lant une transition d'un bottleneck API vers un bottleneck computationnel. Les autres algorithmes pr√©sentent des ratios API/kernel plus favorables, expliquant leurs performances sup√©rieures.
En plus, La premi√®re chose dont nous nous sommes rendus compte est que le transfert de m√©moire √©tait incroyablement inefficace dans nos impl√©mentations na√Øves. Par exemple, dans le **background estimation** nous avons imm√©diatement identifi√© un goulot d'√©tranglement dans les transferts de m√©moire:
**BACKGROUND ESTIMATOR**: CUDA GPU MemOps Summary by Time (`cuda_gpu_mem_time_sum`)
|Operation|Time (%)|Total Time (ns)|Count|
|---|---|---|---|
|[CUDA memcpy Host-to-Device]|52.2|==**145,785,511**==|==**6,796**==|
|[CUDA memcpy Device-to-Host]|47.8|==**133,591,106**==|==**6,796**==|
Lors de l'ex√©cution de `bg_estimator`, pr√®s de 6800 transferts de m√©moire ont lieu par image (HOST-TO-DEVICE et DEVICE-TO-HOST), prenant environ chacun 140 ms √† s‚Äôex√©cuter. Nous avons donc cherch√© √† r√©duire le temps et le nombre d‚Äôop√©rations concernant la m√©moire.
  
Afin de faire cela, plusieurs id√©es sont venues √† l'esprit :
1. **M√©moire Persistente**
2. **M√©moire Partag√©e**
3. **Coalescence de la m√©moire**
### M√©moire Persistente
Afin de r√©duire les co√ªts d'initialisation √† chaque image, nous utilisons une m√©moire persistante en d√©finissant plusieurs de nos variables comme globale qui sera r√©utilis√©e √† chaque image.
**Background estimator**: (no mask cleaning) ‚úÖ
|   |   |   |   |
|---|---|---|---|
|Metric|**Before**|**Now**|**Improvement**|
|**Total Mem Transfers**|6,796|1,699|üìâ~75% fewer transfers|
|**Mem Transfer Time**|~280 ms|~64.4 ms|üìà~76% faster|
|**Kernel Time**|~8.6 ms|~8.7 ms|üìàSlight increase|
**Moving average**: (no mask cleaning) ‚úÖ
|   |   |   |   |
|---|---|---|---|
|Metric|**Before**|**Now**|**Improvement**|
|**Total Mem Transfers (CUDA memcpy Host-to-Device + Device to host each)**|3,398|1,700|üìâ~ 50% fewer trasnfers|
|**Mem Transfer Time (total)**|~130 ms|~64 ms|üìà ~ 50% faster|
|**Kernel Time**|~5.7 ms|~5.7 ms|üìàSlight increase|
Dans l'ensemble, nous avons am√©lior√© l'efficacit√© des transferts de m√©moire.
### M√©moire Partag√©e
Une autre optimisation que nous avons envisag√© d'ajouter √©tait l'utilisation de la m√©moire partag√©e.
Cependant, en examinant les utilisations de la m√©moire partag√©e dans CUDA, nous avons r√©alis√© qu'elle ne correspondait pas √† l'impl√©mentation de notre code. Les r√©sultats obtenus avec Nsight le d√©montrent :
**Background Estimator:** ‚úÖ
|   |   |   |   |
|---|---|---|---|
|Metric|**Before**|**Now**|**Improvement**|
|**Total Mem Transfers**|6,796|6,796|‚ùå no change|
|**Mem Transfer Time**|~270 ms|~273 ms|‚ùå no change|
|**Kernel Time**|~7.8 ms|~9.7 ms|üìâ slower|
Observations g√©n√©rales :
Dans notre cas particulier, l'utilisation de la m√©moire partag√©e n'a pas sembl√© √™tre le meilleur choix. Nos threads traitent pixel par pixel alors que la m√©moire partag√©e est plus utile dans les cas o√π les donn√©es sont r√©utilis√©es et o√π les threads d'un m√™me bloc ont besoin d'acc√©der aux m√™mes donn√©es ou √† des donn√©es voisines. Cela a ajout√© un niveau de difficult√© suppl√©mentaire.
### M√©moire coalescente
Toute notre m√©moire est coalesc√©e, ce qui garantit que les threads au sein d'un warp acc√®dent √† des adresses m√©moire cons√©cutives, minimisant ainsi la latence.
---
==**Version Finale (sans et avec optimisations)**== ‚úÖ
|Dur√©e en ¬µs|==**BG_EST (AVANT)**==|==**BG_EST (APR)**==|==**MOVING AVG (AVANT)**==|==**MOVING AVG (APR√àS)**==|
|---|---|---|---|---|
|CUDA API Summary  <br>(cuda_api_sum)|1126|**230**|519|**436**|
|CUDA GPU Kernel Summary  <br>(cuda_gpu_kern_sum)|**21**|**21**|**20**|**20**|
|CUDA GPU MemOps Summary  <br>(cuda_gpu_mem_time_sum)|227|**102**|122|**85**|
|FPS GPU optimized|728|**2833**|1513|**1848**|
# Conclusion
Ce projet nous a permis de d√©couvrir concr√®tement l‚Äôint√©r√™t de la parall√©lisation GPU dans un contexte de traitement d‚Äôimages en temps r√©el. En partant d‚Äôun algorithme simple de d√©tection de mouvement bas√© sur une moyenne glissante, nous avons explor√©, plusieurs approches compl√©mentaires : background estimator, temporal median, frame differencing, ainsi qu‚Äôune pipeline de nettoyage bas√©e sur la morphologie math√©matique.
La mise en ≈ìuvre de ces algorithmes sur GPU nous a confront√©s √† des probl√©matiques de transferts m√©moire, d‚Äôoptimisation des acc√®s, et de choix d‚Äôarchitecture. L‚Äôutilisation d‚Äôoutils comme Nsight Systems nous a permis d‚Äôidentifier les principaux goulots d‚Äô√©tranglement et d‚Äôam√©liorer consid√©rablement les performances globales, notamment via l‚Äôutilisation de m√©moire persistante.
Enfin, ce projet nous a donn√© une vision claire des compromis √† faire entre pr√©cision, robustesse, et performance. Il nous a √©galement offert une exp√©rience compl√®te, de la conception √† l‚Äôoptimisation, en passant par la validation exp√©rimentale sur des vid√©os r√©elles.
# R√©partition des t√¢ches
||Salom√©|Emma|Paul|Antoine|
|---|---|---|---|---|
|Recherches algorithmes|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Impl√©mentations C++|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Impl√©mentations CUDA|‚ùå|‚úÖ|‚ùå|‚úÖ|
|Optimisations algo CUDA|‚ùå|‚úÖ|‚ùå|‚úÖ|
|Architecture|‚ùå|‚ùå|‚ùå|‚úÖ|
|Recherches morphologie|‚úÖ|‚ùå|‚úÖ|‚ùå|
|Impl√©m. morpho. C++|‚úÖ|‚ùå|‚úÖ|‚ùå|
|Impl√©m. morpho. CUDA|‚ùå|‚ùå|‚úÖ|‚ùå|
|Benchmarks|‚úÖ|‚úÖ|‚ùå|‚ùå|
|**Interpr√©tation**|‚ùå|‚úÖ|‚úÖ|‚úÖ|
|Rapport|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Graphiques|‚úÖ|‚ùå|‚úÖ|‚ùå|
  
# Bibliographie

> [!info] CUDA 4: Profiling CUDA Kernels  
> This is the fifth article in the series I have been writing about Programming in CUDA.  
> [https://medium.com/@rimikadhara/cuda-4-profiling-cuda-kernels-0664252f0ac5](https://medium.com/@rimikadhara/cuda-4-profiling-cuda-kernels-0664252f0ac5)  

> [!info] projet.liris.cnrs.fr  
>  
> [https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1809.pdf](https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1809.pdf)  

> [!info] hal.science  
>  
> [https://hal.science/hal-01222695/document](https://hal.science/hal-01222695/document)  

> [!info] Introduction to Motion Detection: Part 1  
> The easiest way to detect motion with opencv  
> [https://medium.com/@itberrios6/introduction-to-motion-detection-part-1-e031b0bb9bb2](https://medium.com/@itberrios6/introduction-to-motion-detection-part-1-e031b0bb9bb2)  

> [!info] Understanding CUDA Memory Usage: A Practical Guide  
> I understand that learning data science can be really challenging‚Ä¶  
> [https://medium.com/@heyamit10/understanding-cuda-memory-usage-a-practical-guide-6dbb85d4da5a](https://medium.com/@heyamit10/understanding-cuda-memory-usage-a-practical-guide-6dbb85d4da5a)  

> [!info] Introduction to Motion Detection: Part 1  
> The easiest way to detect motion with opencv  
> [https://medium.com/@itberrios6/introduction-to-motion-detection-part-1-e031b0bb9bb2](https://medium.com/@itberrios6/introduction-to-motion-detection-part-1-e031b0bb9bb2)  

> [!info] Motion Detection Based on Frame Difference Method  
> Introduction to Motion Detection  
> [https://medium.com/@200101022/motion-detection-based-on-frame-difference-method-20c2185b7f05](https://medium.com/@200101022/motion-detection-based-on-frame-difference-method-20c2185b7f05)  

> [!info] core.ac.uk  
>  
> [https://core.ac.uk/download/pdf/53189939.pdf](https://core.ac.uk/download/pdf/53189939.pdf)