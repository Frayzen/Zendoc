---
title: Exercices 2
MatiÃ¨re:
  - "[[ProbabilitÃ© et Statistiques]]"
Type: TP/TD
Supports:
  - "[[Feuille2.pdf]]"
---
### ðŸ“˜ **Exercice 3 â€“ Fonction caractÃ©ristique**
### EnoncÃ©
1. DÃ©terminer la **fonction caractÃ©ristique** de la loi de **Bernoulli(p)**.
2. DÃ©terminer la **fonction caractÃ©ristique** de la loi **exponentielle(Î»)**.
---
## âœ… **1. Loi de Bernoulli(p)**
Soit Xâˆ¼Bernoulli(p)X \sim \text{Bernoulli}(p), alors :
P(X=1)=p,P(X=0)=1âˆ’pP(X = 1) = p, \quad P(X = 0) = 1 - p
La **fonction caractÃ©ristique** est :
Ï•X(t)=E(eitX)=(1âˆ’p)â‹…eitâ‹…0+pâ‹…eitâ‹…1=(1âˆ’p)+peit\phi_X(t) = \mathbb{E}\left(e^{itX}\right)  
= (1 - p) \cdot e^{it \cdot 0} + p \cdot e^{it \cdot 1}  
= (1 - p) + p e^{it}
ðŸ”¹ **RÃ©sultat** :
Ï•X(t)=(1âˆ’p)+peit\phi_X(t) = (1 - p) + p e^{it}
---
## âœ… **2. Loi exponentielle(Î»)**
Soit Xâˆ¼Exp(Î»)X \sim \text{Exp}(\lambda) avec densitÃ© :
f(x)=Î»eâˆ’Î»x1xâ‰¥0f(x) = \lambda e^{-\lambda x} \mathbf{1}_{x \geq 0}
La **fonction caractÃ©ristique** est :
Ï•X(t)=E(eitX)=âˆ«0âˆžeitxâ‹…Î»eâˆ’Î»xdx=Î»âˆ«0âˆžeâˆ’(Î»âˆ’it)xdx\phi_X(t) = \mathbb{E}(e^{itX}) = \int_0^{\infty} e^{itx} \cdot \lambda e^{-\lambda x} dx  
= \lambda \int_0^{\infty} e^{-(\lambda - it)x} dx
Cette intÃ©grale converge si Î»>0\lambda > 0. Calcul :
Ï•X(t)=Î»â‹…[1Î»âˆ’it]=Î»Î»âˆ’it\phi_X(t) = \lambda \cdot \left[ \frac{1}{\lambda - it} \right] = \frac{\lambda}{\lambda - it}
ðŸ”¹ **RÃ©sultat** :
Ï•X(t)=Î»Î»âˆ’it\phi_X(t) = \frac{\lambda}{\lambda - it}
---
## ðŸ§  FICHE DE SYNTHÃˆSE â€” Ã€ RETENIR POUR LE PARTIEL
### ðŸ§© DÃ©finitions essentielles
- **Fonction caractÃ©ristique d'une variable alÃ©atoire X** :
    
    Ï•X(t)=E[eitX]\phi_X(t) = \mathbb{E}[e^{itX}]
    
    Elle caractÃ©rise complÃ¨tement la loi de XX et est trÃ¨s utile pour :
    
    - dÃ©terminer la loi limite (via convergence),
    - identifier des lois (ex : loi normale, exponentielle),
    - dÃ©montrer lâ€™indÃ©pendance ou les sommes de variables.
---
### ðŸ’¡ PropriÃ©tÃ©s utiles
- **Bernoulli(p)** :
    
    Ï•X(t)=(1âˆ’p)+peit\phi_X(t) = (1 - p) + p e^{it}
    
- **Exponentielle(Î»)** :
    
    Ï•X(t)=Î»Î»âˆ’it,valideÂ pourÂ tâˆˆR\phi_X(t) = \frac{\lambda}{\lambda - it}, \quad \text{valide pour } t \in \mathbb{R}
    
- La **fonction caractÃ©ristique** est toujours **continue**, **bornÃ©e (â‰¤ 1)**, et Ï•X(0)=1\phi_X(0) = 1.
---
### ðŸŽ¯ ASTUCES
- Si l'Ã©noncÃ© parle de **somme de variables indÃ©pendantes**, **pense Ã  multiplier les fonctions caractÃ©ristiques** :
    
    Ï•X+Y(t)=Ï•X(t)â‹…Ï•Y(t)(siÂ indeËŠpendants)\phi_{X + Y}(t) = \phi_X(t) \cdot \phi_Y(t) \quad \text{(si indÃ©pendants)}
    
- La **fonction caractÃ©ristique dâ€™une loi normale** N(Î¼,Ïƒ2)\mathcal{N}(\mu, \sigma^2) est :
    
    Ï•X(t)=expâ¡(iÎ¼tâˆ’Ïƒ2t22)\phi_X(t) = \exp\left(i \mu t - \frac{\sigma^2 t^2}{2} \right)
    
- Pour la **loi exponentielle**, **attention Ã  ne pas confondre avec la densitÃ©** : ici, l'intÃ©gration se fait avec eitxe^{itx}, ce nâ€™est pas une espÃ©rance standard.
---
Parfait, continuons avec l'**Exercice 4 de la Feuille 2** en gardant le mÃªme format avec la rÃ©solution complÃ¨te et une fiche de synthÃ¨se.
---
### ðŸ“˜ **Exercice 4 â€“ Somme de lois de Bernoulli**
### Ã‰noncÃ©
Montrer que la **somme de n variables alÃ©atoires indÃ©pendantes suivant une loi de Bernoulli de paramÃ¨tre pp** suit une **loi binomiale** de paramÃ¨tres nn et pp.
---
## âœ… **RÃ©solution**
Soient X1,X2,â€¦,XnX_1, X_2, \dots, X_n des variables alÃ©atoires indÃ©pendantes telles que Xiâˆ¼Bernoulli(p)X_i \sim \text{Bernoulli}(p).
On pose :
Sn=X1+X2+â‹¯+XnS_n = X_1 + X_2 + \cdots + X_n
On cherche la loi de SnS_n. Pour tout kâˆˆ{0,1,...,n}k \in \{0, 1, ..., n\}, on veut :
P(Sn=k)P(S_n = k)
Cette probabilitÃ© correspond au fait que **exactement kk des XiX_i valent 1**, et nâˆ’kn - k valent 0. Il y a (nk)\binom{n}{k} faÃ§ons de choisir quels kk XiX_i valent 1, et chaque combinaison a une probabilitÃ© pk(1âˆ’p)nâˆ’kp^k (1 - p)^{n - k}.
Donc :
P(Sn=k)=(nk)pk(1âˆ’p)nâˆ’kP(S_n = k) = \binom{n}{k} p^k (1 - p)^{n - k}
Câ€™est la dÃ©finition mÃªme de la **loi binomiale B(n,p)\mathcal{B}(n, p)**.
---
## ðŸ§  **FICHE DE SYNTHÃˆSE â€“ Ã€ RETENIR POUR LE PARTIEL**
### ðŸ§© DÃ©finitions importantes
- **Loi de Bernoulli(p)** :
    - Xâˆˆ{0,1}X \in \{0, 1\}
    - P(X=1)=pP(X = 1) = p, P(X=0)=1âˆ’pP(X = 0) = 1 - p
- **Loi binomiale B(n,p)\mathcal{B}(n, p)** :
    - ReprÃ©sente le **nombre de succÃ¨s** dans **n Ã©preuves de Bernoulli indÃ©pendantes**
    - P(Sn=k)=(nk)pk(1âˆ’p)nâˆ’kP(S_n = k) = \binom{n}{k} p^k (1 - p)^{n - k}
---
### ðŸ’¡ PropriÃ©tÃ©s utiles
- Si X1,â€¦,XnBernoulli(p)X_1, \dots, X_n \overset{\text{i.i.d.}}{\sim} \text{Bernoulli}(p), alors :
    
    Sn=âˆ‘i=1nXiâˆ¼B(n,p)S_n = \sum_{i=1}^n X_i \sim \mathcal{B}(n, p)
    
- EspÃ©rance : E(Sn)=np\mathbb{E}(S_n) = np
- Variance : Var(Sn)=np(1âˆ’p)\text{Var}(S_n) = np(1 - p)
---
### ðŸŽ¯ ASTUCES
- Ce rÃ©sultat est **fondamental** pour comprendre les **comptages** (succÃ¨s/Ã©checs) dans des expÃ©riences rÃ©pÃ©tÃ©es (tirages, tests, essais cliniques...).
- Il peut Ãªtre **gÃ©nÃ©ralisÃ©** pour des tirages non indÃ©pendants (ex : loi hypergÃ©omÃ©trique), mais la **binomiale suppose indÃ©pendance et identique distribution**.
- En simulation :
    
    ```Python
    import numpy as np
    np.random.binomial(n, p, size=1000)
    ```
    
---
### ðŸ“˜ **Exercice 5 â€“ Somme de deux lois normales**
### EnoncÃ©
Soient deux variables alÃ©atoires indÃ©pendantes X1âˆ¼N(m1,Ïƒ12)X_1 \sim \mathcal{N}(m_1, \sigma_1^2) et X2âˆ¼N(m2,Ïƒ22)X_2 \sim \mathcal{N}(m_2, \sigma_2^2).
Montrer que :
X1+X2âˆ¼N(m1+m2,Ïƒ12+Ïƒ22)X_1 + X_2 \sim \mathcal{N}(m_1 + m_2, \sigma_1^2 + \sigma_2^2)
---
## âœ… **RÃ©solution**
Nous utilisons une **propriÃ©tÃ© fondamentale des lois normales** :

> La somme de deux variables normales indÃ©pendantes est encore une variable normale.
Comme X1X_1 et X2X_2 sont indÃ©pendantes et normales, alors :
- X1+X2X_1 + X_2 suit une **loi normale** (mÃªme sans connaÃ®tre leur densitÃ© explicitement),
- Son espÃ©rance :
    
    E[X1+X2]=E[X1]+E[X2]=m1+m2\mathbb{E}[X_1 + X_2] = \mathbb{E}[X_1] + \mathbb{E}[X_2] = m_1 + m_2
    
- Sa variance (car indÃ©pendantes) :
    
    Var(X1+X2)=Var(X1)+Var(X2)=Ïƒ12+Ïƒ22\text{Var}(X_1 + X_2) = \text{Var}(X_1) + \text{Var}(X_2) = \sigma_1^2 + \sigma_2^2
    
ðŸ”¹ **Conclusion** :
X1+X2âˆ¼N(m1+m2,Ïƒ12+Ïƒ22)X_1 + X_2 \sim \mathcal{N}(m_1 + m_2, \sigma_1^2 + \sigma_2^2)
---
## ðŸ§  **FICHE DE SYNTHÃˆSE â€“ Ã€ RETENIR POUR LE PARTIEL**
### ðŸ§© DÃ©finitions essentielles
- **Loi normale N(m,Ïƒ2)\mathcal{N}(m, \sigma^2)** :
    - DensitÃ© :
        
        f(x)=12Ï€Ïƒ2eâˆ’(xâˆ’m)22Ïƒ2f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{ -\frac{(x - m)^2}{2\sigma^2} }
        
    - EspÃ©rance : E(X)=m\mathbb{E}(X) = m
    - Variance : Var(X)=Ïƒ2\text{Var}(X) = \sigma^2
---
### ðŸ’¡ PropriÃ©tÃ©s fondamentales
- **Somme de deux normales indÃ©pendantes** :
    
    Xâˆ¼N(m1,Ïƒ12),Yâˆ¼N(m2,Ïƒ22)â‡’X+Yâˆ¼N(m1+m2,Ïƒ12+Ïƒ22)X \sim \mathcal{N}(m_1, \sigma_1^2), \quad Y \sim \mathcal{N}(m_2, \sigma_2^2) \Rightarrow X + Y \sim \mathcal{N}(m_1 + m_2, \sigma_1^2 + \sigma_2^2)
    
- Cela se gÃ©nÃ©ralise Ã  la somme de nn normales indÃ©pendantes :
    
    âˆ‘i=1nXiâˆ¼N(âˆ‘mi,âˆ‘Ïƒi2)\sum_{i=1}^n X_i \sim \mathcal{N}\left(\sum m_i, \sum \sigma_i^2\right)
    
---
### ðŸŽ¯ ASTUCES
- **Attention : lâ€™indÃ©pendance est cruciale** pour que les variances sâ€™ajoutent.
- Pour prouver quâ€™une somme suit une normale, on peut aussi :
    - Utiliser les **fonctions caractÃ©ristiques** :
        
        Ï•X+Y(t)=Ï•X(t)â‹…Ï•Y(t)\phi_{X + Y}(t) = \phi_X(t) \cdot \phi_Y(t)
        
    - Ou les **transformations linÃ©aires** dâ€™un vecteur gaussien.
- Ce rÃ©sultat est souvent utilisÃ© en **thÃ©orie des erreurs**, **statistiques**, **infÃ©rences**, ou pour des **modÃ¨les de bruit**.
---
### ðŸ“˜ **Exercice 6 â€“ Minimum de lois exponentielles**
### EnoncÃ©
Soient deux variables alÃ©atoires indÃ©pendantes X1âˆ¼Exp(Î»1)X_1 \sim \text{Exp}(\lambda_1) et X2âˆ¼Exp(Î»2)X_2 \sim \text{Exp}(\lambda_2).
Montrer que :
minâ¡(X1,X2)âˆ¼Exp(Î»1+Î»2)\min(X_1, X_2) \sim \text{Exp}(\lambda_1 + \lambda_2)
---
## âœ… **RÃ©solution**
Soit Z=minâ¡(X1,X2)Z = \min(X_1, X_2).
On cherche la loi de ZZ.
On commence par calculer la **fonction de rÃ©partition** FZ(t)=P(Zâ‰¤t)F_Z(t) = \mathbb{P}(Z \leq t). On utilise le complÃ©ment :
P(Z>t)=P(X1>tÂ etÂ X2>t)=P(X1>t)â‹…P(X2>t)(indeËŠpendance)\mathbb{P}(Z > t) = \mathbb{P}(X_1 > t \text{ et } X_2 > t)  
= \mathbb{P}(X_1 > t) \cdot \mathbb{P}(X_2 > t) \quad \text{(indÃ©pendance)}
Or pour une loi exponentielle :
P(Xi>t)=eâˆ’Î»it\mathbb{P}(X_i > t) = e^{-\lambda_i t}
Donc :
P(Z>t)=eâˆ’Î»1tâ‹…eâˆ’Î»2t=eâˆ’(Î»1+Î»2)t\mathbb{P}(Z > t) = e^{-\lambda_1 t} \cdot e^{-\lambda_2 t} = e^{-(\lambda_1 + \lambda_2)t}
Donc la fonction de rÃ©partition de ZZ est :
FZ(t)=1âˆ’eâˆ’(Î»1+Î»2)tF_Z(t) = 1 - e^{-(\lambda_1 + \lambda_2)t}
Câ€™est la fonction de rÃ©partition dâ€™une loi **exponentielle de paramÃ¨tre Î»1+Î»2\lambda_1 + \lambda_2**.
ðŸ”¹ **Conclusion** :
minâ¡(X1,X2)âˆ¼Exp(Î»1+Î»2)\min(X_1, X_2) \sim \text{Exp}(\lambda_1 + \lambda_2)
---
## ðŸ§  **FICHE DE SYNTHÃˆSE â€“ Ã€ RETENIR POUR LE PARTIEL**
### ðŸ§© DÃ©finitions
- **Loi exponentielle Exp(Î»)\text{Exp}(\lambda)** :
    - DensitÃ© : f(x)=Î»eâˆ’Î»x1xâ‰¥0f(x) = \lambda e^{-\lambda x} \mathbf{1}_{x \geq 0}
    - EspÃ©rance : E(X)=1Î»\mathbb{E}(X) = \frac{1}{\lambda}
    - Variance : Var(X)=1Î»2\text{Var}(X) = \frac{1}{\lambda^2}
---
### ðŸ’¡ PropriÃ©tÃ©s clÃ©s
- Pour deux variables exponentielles **indÃ©pendantes**, la **valeur minimale** suit une **exponentielle de paramÃ¨tre Ã©gal Ã  la somme des paramÃ¨tres** :
    
    minâ¡(X1,X2)âˆ¼Exp(Î»1+Î»2)\min(X_1, X_2) \sim \text{Exp}(\lambda_1 + \lambda_2)
    
- Plus gÃ©nÃ©ralement :
    
    Si X1,â€¦,Xnâˆ¼Exp(Î»i)X_1, \dots, X_n \sim \text{Exp}(\lambda_i), alors :
    
    minâ¡(X1,â€¦,Xn)âˆ¼Exp(Î»1+â‹¯+Î»n)\min(X_1, \dots, X_n) \sim \text{Exp}(\lambda_1 + \dots + \lambda_n)
    
---
### ðŸŽ¯ ASTUCES
- TrÃ¨s utile en **fiabilitÃ©** ou en **file dâ€™attente** : le minimum correspond au **premier Ã©vÃ©nement** qui se produit.
- Cette propriÃ©tÃ© est liÃ©e Ã  la **mÃ©moire sans mÃ©moire** de lâ€™exponentielle :
    
    P(X>t+sâˆ£X>s)=P(X>t)\mathbb{P}(X > t + s \mid X > s) = \mathbb{P}(X > t)
    
- Pour **simuler le temps dâ€™attente jusquâ€™au premier Ã©vÃ©nement parmi plusieurs** :
    - Tirer une exponentielle de paramÃ¨tre Î»1+Î»2+â€¦\lambda_1 + \lambda_2 + \dots
    - Choisir lâ€™origine du minimum de maniÃ¨re alÃ©atoire pondÃ©rÃ©e (probas proportionnelles aux Î»)
---
### ðŸ“˜ **Exercice 11 â€” Une loi faible des grands nombres**
### Ã‰noncÃ©
Soit (Xn)nâ‰¥1(X_n)_{n \geq 1} une suite de variables alÃ©atoires :
- Dans L2L^2 (câ€™est-Ã -dire avec variance finie),
- **CentrÃ©es** : E(Xn)=0\mathbb{E}(X_n) = 0,
- **Deux Ã  deux non corrÃ©lÃ©es** : Cov(Xi,Xj)=0\text{Cov}(X_i, X_j) = 0 pour iâ‰ ji \neq j.
On dÃ©finit :
Sn=âˆ‘k=1nXkS_n = \sum_{k=1}^n X_k
Et on suppose :
1nâˆ‘k=1nVar(Xk)âŸ¶0\frac{1}{n} \sum_{k=1}^n \text{Var}(X_k) \longrightarrow 0
Montrer que :
SnnâŸ¶0enÂ moyenneÂ quadratique\frac{S_n}{n} \longrightarrow 0 \quad \text{en moyenne quadratique}
---
## âœ… **RÃ©solution dÃ©taillÃ©e**
On veut montrer que :
E[(Snn)2]â†’nâ†’âˆž0\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] \xrightarrow[n \to \infty]{} 0
Or, comme E[Xk]=0\mathbb{E}[X_k] = 0, on a :
E[(Snn)2]=1n2E[Sn2]=1n2Var(Sn)\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] = \frac{1}{n^2} \mathbb{E}[S_n^2] = \frac{1}{n^2} \text{Var}(S_n)
Comme les XkX_k sont deux Ã  deux non corrÃ©lÃ©es, alors :
Var(Sn)=âˆ‘k=1nVar(Xk)\text{Var}(S_n) = \sum_{k=1}^n \text{Var}(X_k)
Donc :
E[(Snn)2]=1n2âˆ‘k=1nVar(Xk)=1nâ‹…(1nâˆ‘k=1nVar(Xk))\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] = \frac{1}{n^2} \sum_{k=1}^n \text{Var}(X_k) = \frac{1}{n} \cdot \left( \frac{1}{n} \sum_{k=1}^n \text{Var}(X_k) \right)
Et lâ€™hypothÃ¨se de lâ€™exercice dit que :
1nâˆ‘k=1nVar(Xk)âŸ¶0\frac{1}{n} \sum_{k=1}^n \text{Var}(X_k) \longrightarrow 0
Donc :
E[(Snn)2]âŸ¶0\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] \longrightarrow 0
ðŸ”¹ **Conclusion** :
Snnâ†’nâ†’âˆžL20\frac{S_n}{n} \xrightarrow[n \to \infty]{L^2} 0
---
## ðŸ§  **FICHE DE SYNTHÃˆSE â€“ Ã€ RETENIR POUR LE PARTIEL**
### ðŸ§© DÃ©finitions clÃ©s
- **Convergence en moyenne quadratique (LÂ²)** :
    
    Une suite Xnâ†’XX_n \to X en L2L^2 si :
    
    E[(Xnâˆ’X)2]â†’0\mathbb{E}\left[(X_n - X)^2\right] \to 0
    
- **Non-corrÃ©lation** :
    
    Cov(X,Y)=0\text{Cov}(X, Y) = 0 (nâ€™implique pas indÃ©pendance, sauf en loi normale).
    
---
### ðŸ“Œ PropriÃ©tÃ© illustrÃ©e ici (loi faible des grands nombres modifiÃ©e)

> Si (Xn)âˆˆL2(X_n) \in L^2, centrÃ©es, deux Ã  deux non corrÃ©lÃ©es, et
> 
> 1nâˆ‘Var(Xk)âŸ¶0,\frac{1}{n} \sum \text{Var}(X_k) \longrightarrow 0,
> 
> alors :
> 
> Snnâ†’L20\frac{S_n}{n} \xrightarrow{L^2} 0
---
### ðŸŽ¯ ASTUCES
- La mÃ©thode ici repose uniquement sur la **variance** de la somme et lâ€™**inÃ©galitÃ© de BienaymÃ©** (utilisÃ©e implicitement via le calcul dâ€™espÃ©rance quadratique).
- Cela montre quâ€™un comportement "moyen" disparaÃ®t si la **variance moyenne** tend vers zÃ©ro :
    
    â†’ **Pas besoin dâ€™indÃ©pendance**, seulement **non-corrÃ©lation**.
    
- Dans un contexte de partiel, toujours vÃ©rifier :
    - **Centrage** : E[Xn]=0\mathbb{E}[X_n] = 0,
    - **Var finie** : XnâˆˆL2X_n \in L^2,
    - **(Non)-corrÃ©lation** ou **indÃ©pendance**.
---
### ðŸ“˜ **Exercice 14 â€” Vecteurs gaussiens et transformation alÃ©atoire**
![[exercice14f2.pdf]]
### Contexte
Soient :
- Xâˆ¼N(0,1)X \sim \mathcal{N}(0,1) (loi normale centrÃ©e rÃ©duite),
- Îµâˆˆ{âˆ’1,1}\varepsilon \in \{-1, 1\} telle que P(Îµ=âˆ’1)=P(Îµ=1)=12\mathbb{P}(\varepsilon = -1) = \mathbb{P}(\varepsilon = 1) = \frac{1}{2},
- Îµ\varepsilon et XX indÃ©pendants,
Et on dÃ©finit :
Y:=ÎµXY := \varepsilon X
---
### âœ… RÃ©solution dÃ©taillÃ©e
### **1. Montrer que Yâˆ¼N(0,1)Y \sim \mathcal{N}(0,1)**
On utilise la **fonction de rÃ©partition** :
FY(x)=P(ÎµXâ‰¤x)=12P(âˆ’Xâ‰¤x)+12P(Xâ‰¤x)=12P(Xâ‰¥âˆ’x)+12P(Xâ‰¤x)F_Y(x) = \mathbb{P}(\varepsilon X \leq x)  
= \frac{1}{2} \mathbb{P}(-X \leq x) + \frac{1}{2} \mathbb{P}(X \leq x)  
= \frac{1}{2} \mathbb{P}(X \geq -x) + \frac{1}{2} \mathbb{P}(X \leq x)
La loi normale centrÃ©e est **symÃ©trique**, donc P(Xâ‰¤x)=P(Xâ‰¥âˆ’x)\mathbb{P}(X \leq x) = \mathbb{P}(X \geq -x).
Ainsi :
FY(x)=P(Xâ‰¤x)â‡’Yâˆ¼N(0,1)F_Y(x) = \mathbb{P}(X \leq x)  
\Rightarrow Y \sim \mathcal{N}(0,1)
---
### **2. Calculer Cov(X,Y)\text{Cov}(X, Y)**
Cov(X,Y)=E[XY]âˆ’E[X]E[Y]=E[ÎµX2]\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = \mathbb{E}[\varepsilon X^2]
Par indÃ©pendance :
E[ÎµX2]=E[Îµ]â‹…E[X2]=0â‹…1=0â‡’Cov(X,Y)=0\mathbb{E}[\varepsilon X^2] = \mathbb{E}[\varepsilon] \cdot \mathbb{E}[X^2] = 0 \cdot 1 = 0  
\Rightarrow \text{Cov}(X, Y) = 0
---
### **3. DÃ©terminer la loi de Z:=X+YZ := X + Y**
Deux cas :
- Si Îµ=1\varepsilon = 1 : Z=2Xâˆ¼N(0,4)Z = 2X \sim \mathcal{N}(0, 4)
- Si Îµ=âˆ’1\varepsilon = -1 : Z=0Z = 0
Donc ZZ prend la valeur 0 avec probabilitÃ© 12\frac{1}{2}, et suit une loi N(0,4)\mathcal{N}(0,4) avec probabilitÃ© 12\frac{1}{2}.
Donc la fonction de rÃ©partition est :
FZ(z)={12F2X(z)siÂ z<012+12F2X(z)siÂ zâ‰¥0F_Z(z) =  
\begin{cases}  
\frac{1}{2} F_{2X}(z) & \text{si } z < 0 \\  
\frac{1}{2} + \frac{1}{2} F_{2X}(z) & \text{si } z \geq 0  
\end{cases}
Cette fonction **prÃ©sente un saut en 0**, donc **ZZ nâ€™a pas de densitÃ©**.
---
### **4. (X,Y)T(X, Y)^T nâ€™est pas un vecteur gaussien**
Car **X+YX + Y** nâ€™a pas une loi normale â†’ contradiction avec la dÃ©finition dâ€™un **vecteur gaussien**, qui impose que **toute combinaison linÃ©aire** suive une loi normale.
---
## ðŸ§  FICHE DE SYNTHÃˆSE â€“ POUR LE PARTIEL
### ðŸ§© DÃ©finitions clÃ©s
- **Vecteur gaussien** : (X1,...,Xn)T(X_1, ..., X_n)^T est gaussien si **toute combinaison linÃ©aire** a1X1+â‹¯+anXna_1 X_1 + \dots + a_n X_n est une variable normale.
- **Loi symÃ©trique** : Xâˆ¼N(0,1)â‡’Xâˆ’XX \sim \mathcal{N}(0,1) \Rightarrow X \overset{d}{=} -X
- **Covariance** :
    
    Cov(X,Y)=E[XY]âˆ’E[X]E[Y]\text{Cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y]
    
---
### ðŸ’¡ PropriÃ©tÃ©s utiles
- Si Îµâˆˆ{âˆ’1,1}\varepsilon \in \{-1,1\} indÃ©pendante de Xâˆ¼N(0,1)X \sim \mathcal{N}(0,1), alors ÎµXâˆ¼N(0,1)\varepsilon X \sim \mathcal{N}(0,1)
- Cov(X,ÎµX)=E[Îµ]â‹…E[X2]\text{Cov}(X, \varepsilon X) = \mathbb{E}[\varepsilon] \cdot \mathbb{E}[X^2], donc = 0 si E[Îµ]=0\mathbb{E}[\varepsilon] = 0
- Si une **combinaison linÃ©aire** nâ€™est pas normale, le vecteur nâ€™est **pas gaussien**
---
### ðŸŽ¯ ASTUCES
- VÃ©rifie bien si **toute combinaison linÃ©aire** est normale pour justifier quâ€™un vecteur est gaussien.
- Attention : **covariance nulle â‰  indÃ©pendance**, sauf cas normal.
- Si tu trouves une variable qui prend **une valeur fixe avec proba >0> 0** + densitÃ© ailleurs â†’ ce nâ€™est **ni discret ni continu**.
---
### ðŸ“˜ **Exercice 15 â€” Vecteur gaussien et indÃ©pendance**
### EnoncÃ©
Soient XX et YY deux variables alÃ©atoires **indÃ©pendantes**, toutes deux suivant une loi normale centrÃ©e rÃ©duite :
X,Yâˆ¼N(0,1)X, Y \sim \mathcal{N}(0,1)
On dÃ©finit :
- U=X+YU = X + Y
- V=Xâˆ’YV = X - Y
On demande :
1. Montrer que (U,V)T(U, V)^T est un **vecteur gaussien**
2. Montrer que UU et VV sont **indÃ©pendantes**
---
## âœ… **RÃ©solution**
### 1. Montrer que (U,V)T(U, V)^T est un vecteur gaussien
Rappel : un vecteur alÃ©atoire est **gaussien** si **toute combinaison linÃ©aire** des composantes suit une **loi normale**.
On a :
U=X+Y,V=Xâˆ’YU = X + Y, \quad V = X - Y
Or XX et YY sont **indÃ©pendantes** et **normales** â‡’ le vecteur (X,Y)T(X, Y)^T est gaussien.
Et toute **application linÃ©aire** dâ€™un vecteur gaussien reste gaussienne.
Donc :
(U,V)T=Aâ‹…(X,Y)T,ouË‹Â A=(111âˆ’1)(U, V)^T = A \cdot (X, Y)^T, \quad \text{oÃ¹ } A =  
\begin{pmatrix}  
1 & 1 \\  
1 & -1  
\end{pmatrix}
ðŸ”¹ **Conclusion** :
(U,V)TÂ estÂ unÂ vecteurÂ gaussien(U, V)^T \text{ est un vecteur gaussien}
---
### 2. Montrer que UU et VV sont indÃ©pendantes
Pour deux variables **jointement normales**, **lâ€™indÃ©pendance Ã©quivaut Ã  la non-corrÃ©lation**.
On calcule donc Cov(U,V)\text{Cov}(U, V).
### a) EspÃ©rances :
E[U]=E[X+Y]=0,E[V]=E[Xâˆ’Y]=0\mathbb{E}[U] = \mathbb{E}[X + Y] = 0, \quad \mathbb{E}[V] = \mathbb{E}[X - Y] = 0
### b) Covariance :
Cov(U,V)=E[UV]=E[(X+Y)(Xâˆ’Y)]=E[X2âˆ’Y2]\text{Cov}(U, V) = \mathbb{E}[UV] = \mathbb{E}[(X+Y)(X-Y)] = \mathbb{E}[X^2 - Y^2]
Comme Xâˆ¼N(0,1)X \sim \mathcal{N}(0,1) et Yâˆ¼N(0,1)Y \sim \mathcal{N}(0,1) :
E[X2]=E[Y2]=1â‡’Cov(U,V)=1âˆ’1=0\mathbb{E}[X^2] = \mathbb{E}[Y^2] = 1  
\Rightarrow \text{Cov}(U, V) = 1 - 1 = 0
ðŸ”¹ UU et VV sont **non corrÃ©lÃ©es**, et comme elles sont **jointement normales**, cela implique qu'elles sont **indÃ©pendantes**.
---
## ðŸ§  FICHE DE SYNTHÃˆSE â€“ Ã€ RETENIR POUR LE PARTIEL
### ðŸ§© Notions clÃ©s
- **Vecteur gaussien** : toute combinaison linÃ©aire de ses composantes suit une loi normale.
- **Application linÃ©aire dâ€™un vecteur gaussien** â†’ donne encore un vecteur gaussien.
- **IndÃ©pendance pour des lois normales** :
    - Deux variables normales **jointement normales** sont **indÃ©pendantes si et seulement si elles sont non corrÃ©lÃ©es**.
---
### ðŸ’¡ PropriÃ©tÃ©s utiles
Soit X,Yâˆ¼N(0,1)X, Y \sim \mathcal{N}(0,1) indÃ©pendantes :
- U=X+Yâˆ¼N(0,2)U = X + Y \sim \mathcal{N}(0,2)
- V=Xâˆ’Yâˆ¼N(0,2)V = X - Y \sim \mathcal{N}(0,2)
- Cov(U,V)=0â‡’UâŠ¥V\text{Cov}(U, V) = 0 \Rightarrow U \perp V
---
### ðŸŽ¯ ASTUCES
- Si on te demande si un **vecteur est gaussien**, vÃ©rifie si câ€™est une **transformation linÃ©aire** dâ€™un vecteur de normales.
- Si tu montres que deux variables normales sont **non corrÃ©lÃ©es**, et quâ€™elles viennent dâ€™un **vecteur gaussien**, alors elles sont **automatiquement indÃ©pendantes**.
- Ã€ lâ€™oral ou Ã  lâ€™Ã©crit, **insiste bien sur lâ€™implication "normales + non corrÃ©lÃ©es â‡’ indÃ©pendantes"** (ce nâ€™est pas vrai en gÃ©nÃ©ral).
---
Souhaites-tu quâ€™on fasse lâ€™exercice 16 aussi ? Câ€™est la suite logique.