---
title: Exercices 2
Mati√®re:
  - "[[Probabilit√© et Statistiques]]"
Type: TP/TD
Supports:
  - "[[Feuille2.pdf]]"
---
### üìò **Exercice 3 ‚Äì Fonction caract√©ristique**
### Enonc√©
1. D√©terminer la **fonction caract√©ristique** de la loi de **Bernoulli(p)**.
2. D√©terminer la **fonction caract√©ristique** de la loi **exponentielle(Œª)**.
---
## ‚úÖ **1. Loi de Bernoulli(p)**
Soit X‚àºBernoulli(p)X \sim \text{Bernoulli}(p), alors :
P(X=1)=p,P(X=0)=1‚àípP(X = 1) = p, \quad P(X = 0) = 1 - p
La **fonction caract√©ristique** est :
œïX(t)=E(eitX)=(1‚àíp)‚ãÖeit‚ãÖ0+p‚ãÖeit‚ãÖ1=(1‚àíp)+peit\phi_X(t) = \mathbb{E}\left(e^{itX}\right)  
= (1 - p) \cdot e^{it \cdot 0} + p \cdot e^{it \cdot 1}  
= (1 - p) + p e^{it}
üîπ **R√©sultat** :
œïX(t)=(1‚àíp)+peit\phi_X(t) = (1 - p) + p e^{it}
---
## ‚úÖ **2. Loi exponentielle(Œª)**
Soit X‚àºExp(Œª)X \sim \text{Exp}(\lambda) avec densit√© :
f(x)=Œªe‚àíŒªx1x‚â•0f(x) = \lambda e^{-\lambda x} \mathbf{1}_{x \geq 0}
La **fonction caract√©ristique** est :
œïX(t)=E(eitX)=‚à´0‚àûeitx‚ãÖŒªe‚àíŒªxdx=Œª‚à´0‚àûe‚àí(Œª‚àíit)xdx\phi_X(t) = \mathbb{E}(e^{itX}) = \int_0^{\infty} e^{itx} \cdot \lambda e^{-\lambda x} dx  
= \lambda \int_0^{\infty} e^{-(\lambda - it)x} dx
Cette int√©grale converge si Œª>0\lambda > 0. Calcul :
œïX(t)=Œª‚ãÖ[1Œª‚àíit]=ŒªŒª‚àíit\phi_X(t) = \lambda \cdot \left[ \frac{1}{\lambda - it} \right] = \frac{\lambda}{\lambda - it}
üîπ **R√©sultat** :
œïX(t)=ŒªŒª‚àíit\phi_X(t) = \frac{\lambda}{\lambda - it}
---
## üß† FICHE DE SYNTH√àSE ‚Äî √Ä RETENIR POUR LE PARTIEL
### üß© D√©finitions essentielles
- **Fonction caract√©ristique d'une variable al√©atoire X** :
    
    œïX(t)=E[eitX]\phi_X(t) = \mathbb{E}[e^{itX}]
    
    Elle caract√©rise compl√®tement la loi de XX et est tr√®s utile pour :
    
    - d√©terminer la loi limite (via convergence),
    - identifier des lois (ex : loi normale, exponentielle),
    - d√©montrer l‚Äôind√©pendance ou les sommes de variables.
---
### üí° Propri√©t√©s utiles
- **Bernoulli(p)** :
    
    œïX(t)=(1‚àíp)+peit\phi_X(t) = (1 - p) + p e^{it}
    
- **Exponentielle(Œª)** :
    
    œïX(t)=ŒªŒª‚àíit,valide¬†pour¬†t‚ààR\phi_X(t) = \frac{\lambda}{\lambda - it}, \quad \text{valide pour } t \in \mathbb{R}
    
- La **fonction caract√©ristique** est toujours **continue**, **born√©e (‚â§ 1)**, et œïX(0)=1\phi_X(0) = 1.
---
### üéØ ASTUCES
- Si l'√©nonc√© parle de **somme de variables ind√©pendantes**, **pense √† multiplier les fonctions caract√©ristiques** :
    
    œïX+Y(t)=œïX(t)‚ãÖœïY(t)(si¬†indeÀäpendants)\phi_{X + Y}(t) = \phi_X(t) \cdot \phi_Y(t) \quad \text{(si ind√©pendants)}
    
- La **fonction caract√©ristique d‚Äôune loi normale** N(Œº,œÉ2)\mathcal{N}(\mu, \sigma^2) est :
    
    œïX(t)=exp‚Å°(iŒºt‚àíœÉ2t22)\phi_X(t) = \exp\left(i \mu t - \frac{\sigma^2 t^2}{2} \right)
    
- Pour la **loi exponentielle**, **attention √† ne pas confondre avec la densit√©** : ici, l'int√©gration se fait avec eitxe^{itx}, ce n‚Äôest pas une esp√©rance standard.
---
Parfait, continuons avec l'**Exercice 4 de la Feuille 2** en gardant le m√™me format avec la r√©solution compl√®te et une fiche de synth√®se.
---
### üìò **Exercice 4 ‚Äì Somme de lois de Bernoulli**
### √ânonc√©
Montrer que la **somme de n variables al√©atoires ind√©pendantes suivant une loi de Bernoulli de param√®tre pp** suit une **loi binomiale** de param√®tres nn et pp.
---
## ‚úÖ **R√©solution**
Soient X1,X2,‚Ä¶,XnX_1, X_2, \dots, X_n des variables al√©atoires ind√©pendantes telles que Xi‚àºBernoulli(p)X_i \sim \text{Bernoulli}(p).
On pose :
Sn=X1+X2+‚ãØ+XnS_n = X_1 + X_2 + \cdots + X_n
On cherche la loi de SnS_n. Pour tout k‚àà{0,1,...,n}k \in \{0, 1, ..., n\}, on veut :
P(Sn=k)P(S_n = k)
Cette probabilit√© correspond au fait que **exactement kk des XiX_i valent 1**, et n‚àíkn - k valent 0. Il y a (nk)\binom{n}{k} fa√ßons de choisir quels kk XiX_i valent 1, et chaque combinaison a une probabilit√© pk(1‚àíp)n‚àíkp^k (1 - p)^{n - k}.
Donc :
P(Sn=k)=(nk)pk(1‚àíp)n‚àíkP(S_n = k) = \binom{n}{k} p^k (1 - p)^{n - k}
C‚Äôest la d√©finition m√™me de la **loi binomiale B(n,p)\mathcal{B}(n, p)**.
---
## üß† **FICHE DE SYNTH√àSE ‚Äì √Ä RETENIR POUR LE PARTIEL**
### üß© D√©finitions importantes
- **Loi de Bernoulli(p)** :
    - X‚àà{0,1}X \in \{0, 1\}
    - P(X=1)=pP(X = 1) = p, P(X=0)=1‚àípP(X = 0) = 1 - p
- **Loi binomiale B(n,p)\mathcal{B}(n, p)** :
    - Repr√©sente le **nombre de succ√®s** dans **n √©preuves de Bernoulli ind√©pendantes**
    - P(Sn=k)=(nk)pk(1‚àíp)n‚àíkP(S_n = k) = \binom{n}{k} p^k (1 - p)^{n - k}
---
### üí° Propri√©t√©s utiles
- Si X1,‚Ä¶,XnBernoulli(p)X_1, \dots, X_n \overset{\text{i.i.d.}}{\sim} \text{Bernoulli}(p), alors :
    
    Sn=‚àëi=1nXi‚àºB(n,p)S_n = \sum_{i=1}^n X_i \sim \mathcal{B}(n, p)
    
- Esp√©rance : E(Sn)=np\mathbb{E}(S_n) = np
- Variance : Var(Sn)=np(1‚àíp)\text{Var}(S_n) = np(1 - p)
---
### üéØ ASTUCES
- Ce r√©sultat est **fondamental** pour comprendre les **comptages** (succ√®s/√©checs) dans des exp√©riences r√©p√©t√©es (tirages, tests, essais cliniques...).
- Il peut √™tre **g√©n√©ralis√©** pour des tirages non ind√©pendants (ex : loi hyperg√©om√©trique), mais la **binomiale suppose ind√©pendance et identique distribution**.
- En simulation :
    
    ```Python
    import numpy as np
    np.random.binomial(n, p, size=1000)
    ```
    
---
### üìò **Exercice 5 ‚Äì Somme de deux lois normales**
### Enonc√©
Soient deux variables al√©atoires ind√©pendantes X1‚àºN(m1,œÉ12)X_1 \sim \mathcal{N}(m_1, \sigma_1^2) et X2‚àºN(m2,œÉ22)X_2 \sim \mathcal{N}(m_2, \sigma_2^2).
Montrer que :
X1+X2‚àºN(m1+m2,œÉ12+œÉ22)X_1 + X_2 \sim \mathcal{N}(m_1 + m_2, \sigma_1^2 + \sigma_2^2)
---
## ‚úÖ **R√©solution**
Nous utilisons une **propri√©t√© fondamentale des lois normales** :

> La somme de deux variables normales ind√©pendantes est encore une variable normale.
Comme X1X_1 et X2X_2 sont ind√©pendantes et normales, alors :
- X1+X2X_1 + X_2 suit une **loi normale** (m√™me sans conna√Ætre leur densit√© explicitement),
- Son esp√©rance :
    
    E[X1+X2]=E[X1]+E[X2]=m1+m2\mathbb{E}[X_1 + X_2] = \mathbb{E}[X_1] + \mathbb{E}[X_2] = m_1 + m_2
    
- Sa variance (car ind√©pendantes) :
    
    Var(X1+X2)=Var(X1)+Var(X2)=œÉ12+œÉ22\text{Var}(X_1 + X_2) = \text{Var}(X_1) + \text{Var}(X_2) = \sigma_1^2 + \sigma_2^2
    
üîπ **Conclusion** :
X1+X2‚àºN(m1+m2,œÉ12+œÉ22)X_1 + X_2 \sim \mathcal{N}(m_1 + m_2, \sigma_1^2 + \sigma_2^2)
---
## üß† **FICHE DE SYNTH√àSE ‚Äì √Ä RETENIR POUR LE PARTIEL**
### üß© D√©finitions essentielles
- **Loi normale N(m,œÉ2)\mathcal{N}(m, \sigma^2)** :
    - Densit√© :
        
        f(x)=12œÄœÉ2e‚àí(x‚àím)22œÉ2f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{ -\frac{(x - m)^2}{2\sigma^2} }
        
    - Esp√©rance : E(X)=m\mathbb{E}(X) = m
    - Variance : Var(X)=œÉ2\text{Var}(X) = \sigma^2
---
### üí° Propri√©t√©s fondamentales
- **Somme de deux normales ind√©pendantes** :
    
    X‚àºN(m1,œÉ12),Y‚àºN(m2,œÉ22)‚áíX+Y‚àºN(m1+m2,œÉ12+œÉ22)X \sim \mathcal{N}(m_1, \sigma_1^2), \quad Y \sim \mathcal{N}(m_2, \sigma_2^2) \Rightarrow X + Y \sim \mathcal{N}(m_1 + m_2, \sigma_1^2 + \sigma_2^2)
    
- Cela se g√©n√©ralise √† la somme de nn normales ind√©pendantes :
    
    ‚àëi=1nXi‚àºN(‚àëmi,‚àëœÉi2)\sum_{i=1}^n X_i \sim \mathcal{N}\left(\sum m_i, \sum \sigma_i^2\right)
    
---
### üéØ ASTUCES
- **Attention : l‚Äôind√©pendance est cruciale** pour que les variances s‚Äôajoutent.
- Pour prouver qu‚Äôune somme suit une normale, on peut aussi :
    - Utiliser les **fonctions caract√©ristiques** :
        
        œïX+Y(t)=œïX(t)‚ãÖœïY(t)\phi_{X + Y}(t) = \phi_X(t) \cdot \phi_Y(t)
        
    - Ou les **transformations lin√©aires** d‚Äôun vecteur gaussien.
- Ce r√©sultat est souvent utilis√© en **th√©orie des erreurs**, **statistiques**, **inf√©rences**, ou pour des **mod√®les de bruit**.
---
### üìò **Exercice 6 ‚Äì Minimum de lois exponentielles**
### Enonc√©
Soient deux variables al√©atoires ind√©pendantes X1‚àºExp(Œª1)X_1 \sim \text{Exp}(\lambda_1) et X2‚àºExp(Œª2)X_2 \sim \text{Exp}(\lambda_2).
Montrer que :
min‚Å°(X1,X2)‚àºExp(Œª1+Œª2)\min(X_1, X_2) \sim \text{Exp}(\lambda_1 + \lambda_2)
---
## ‚úÖ **R√©solution**
Soit Z=min‚Å°(X1,X2)Z = \min(X_1, X_2).
On cherche la loi de ZZ.
On commence par calculer la **fonction de r√©partition** FZ(t)=P(Z‚â§t)F_Z(t) = \mathbb{P}(Z \leq t). On utilise le compl√©ment :
P(Z>t)=P(X1>t¬†et¬†X2>t)=P(X1>t)‚ãÖP(X2>t)(indeÀäpendance)\mathbb{P}(Z > t) = \mathbb{P}(X_1 > t \text{ et } X_2 > t)  
= \mathbb{P}(X_1 > t) \cdot \mathbb{P}(X_2 > t) \quad \text{(ind√©pendance)}
Or pour une loi exponentielle :
P(Xi>t)=e‚àíŒªit\mathbb{P}(X_i > t) = e^{-\lambda_i t}
Donc :
P(Z>t)=e‚àíŒª1t‚ãÖe‚àíŒª2t=e‚àí(Œª1+Œª2)t\mathbb{P}(Z > t) = e^{-\lambda_1 t} \cdot e^{-\lambda_2 t} = e^{-(\lambda_1 + \lambda_2)t}
Donc la fonction de r√©partition de ZZ est :
FZ(t)=1‚àíe‚àí(Œª1+Œª2)tF_Z(t) = 1 - e^{-(\lambda_1 + \lambda_2)t}
C‚Äôest la fonction de r√©partition d‚Äôune loi **exponentielle de param√®tre Œª1+Œª2\lambda_1 + \lambda_2**.
üîπ **Conclusion** :
min‚Å°(X1,X2)‚àºExp(Œª1+Œª2)\min(X_1, X_2) \sim \text{Exp}(\lambda_1 + \lambda_2)
---
## üß† **FICHE DE SYNTH√àSE ‚Äì √Ä RETENIR POUR LE PARTIEL**
### üß© D√©finitions
- **Loi exponentielle Exp(Œª)\text{Exp}(\lambda)** :
    - Densit√© : f(x)=Œªe‚àíŒªx1x‚â•0f(x) = \lambda e^{-\lambda x} \mathbf{1}_{x \geq 0}
    - Esp√©rance : E(X)=1Œª\mathbb{E}(X) = \frac{1}{\lambda}
    - Variance : Var(X)=1Œª2\text{Var}(X) = \frac{1}{\lambda^2}
---
### üí° Propri√©t√©s cl√©s
- Pour deux variables exponentielles **ind√©pendantes**, la **valeur minimale** suit une **exponentielle de param√®tre √©gal √† la somme des param√®tres** :
    
    min‚Å°(X1,X2)‚àºExp(Œª1+Œª2)\min(X_1, X_2) \sim \text{Exp}(\lambda_1 + \lambda_2)
    
- Plus g√©n√©ralement :
    
    Si X1,‚Ä¶,Xn‚àºExp(Œªi)X_1, \dots, X_n \sim \text{Exp}(\lambda_i), alors :
    
    min‚Å°(X1,‚Ä¶,Xn)‚àºExp(Œª1+‚ãØ+Œªn)\min(X_1, \dots, X_n) \sim \text{Exp}(\lambda_1 + \dots + \lambda_n)
    
---
### üéØ ASTUCES
- Tr√®s utile en **fiabilit√©** ou en **file d‚Äôattente** : le minimum correspond au **premier √©v√©nement** qui se produit.
- Cette propri√©t√© est li√©e √† la **m√©moire sans m√©moire** de l‚Äôexponentielle :
    
    P(X>t+s‚à£X>s)=P(X>t)\mathbb{P}(X > t + s \mid X > s) = \mathbb{P}(X > t)
    
- Pour **simuler le temps d‚Äôattente jusqu‚Äôau premier √©v√©nement parmi plusieurs** :
    - Tirer une exponentielle de param√®tre Œª1+Œª2+‚Ä¶\lambda_1 + \lambda_2 + \dots
    - Choisir l‚Äôorigine du minimum de mani√®re al√©atoire pond√©r√©e (probas proportionnelles aux Œª)
---
### üìò **Exercice 11 ‚Äî Une loi faible des grands nombres**
### √ânonc√©
Soit (Xn)n‚â•1(X_n)_{n \geq 1} une suite de variables al√©atoires :
- Dans L2L^2 (c‚Äôest-√†-dire avec variance finie),
- **Centr√©es** : E(Xn)=0\mathbb{E}(X_n) = 0,
- **Deux √† deux non corr√©l√©es** : Cov(Xi,Xj)=0\text{Cov}(X_i, X_j) = 0 pour i‚â†ji \neq j.
On d√©finit :
Sn=‚àëk=1nXkS_n = \sum_{k=1}^n X_k
Et on suppose :
1n‚àëk=1nVar(Xk)‚ü∂0\frac{1}{n} \sum_{k=1}^n \text{Var}(X_k) \longrightarrow 0
Montrer que :
Snn‚ü∂0en¬†moyenne¬†quadratique\frac{S_n}{n} \longrightarrow 0 \quad \text{en moyenne quadratique}
---
## ‚úÖ **R√©solution d√©taill√©e**
On veut montrer que :
E[(Snn)2]‚Üín‚Üí‚àû0\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] \xrightarrow[n \to \infty]{} 0
Or, comme E[Xk]=0\mathbb{E}[X_k] = 0, on a :
E[(Snn)2]=1n2E[Sn2]=1n2Var(Sn)\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] = \frac{1}{n^2} \mathbb{E}[S_n^2] = \frac{1}{n^2} \text{Var}(S_n)
Comme les XkX_k sont deux √† deux non corr√©l√©es, alors :
Var(Sn)=‚àëk=1nVar(Xk)\text{Var}(S_n) = \sum_{k=1}^n \text{Var}(X_k)
Donc :
E[(Snn)2]=1n2‚àëk=1nVar(Xk)=1n‚ãÖ(1n‚àëk=1nVar(Xk))\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] = \frac{1}{n^2} \sum_{k=1}^n \text{Var}(X_k) = \frac{1}{n} \cdot \left( \frac{1}{n} \sum_{k=1}^n \text{Var}(X_k) \right)
Et l‚Äôhypoth√®se de l‚Äôexercice dit que :
1n‚àëk=1nVar(Xk)‚ü∂0\frac{1}{n} \sum_{k=1}^n \text{Var}(X_k) \longrightarrow 0
Donc :
E[(Snn)2]‚ü∂0\mathbb{E}\left[\left( \frac{S_n}{n} \right)^2\right] \longrightarrow 0
üîπ **Conclusion** :
Snn‚Üín‚Üí‚àûL20\frac{S_n}{n} \xrightarrow[n \to \infty]{L^2} 0
---
## üß† **FICHE DE SYNTH√àSE ‚Äì √Ä RETENIR POUR LE PARTIEL**
### üß© D√©finitions cl√©s
- **Convergence en moyenne quadratique (L¬≤)** :
    
    Une suite Xn‚ÜíXX_n \to X en L2L^2 si :
    
    E[(Xn‚àíX)2]‚Üí0\mathbb{E}\left[(X_n - X)^2\right] \to 0
    
- **Non-corr√©lation** :
    
    Cov(X,Y)=0\text{Cov}(X, Y) = 0 (n‚Äôimplique pas ind√©pendance, sauf en loi normale).
    
---
### üìå Propri√©t√© illustr√©e ici (loi faible des grands nombres modifi√©e)

> Si (Xn)‚ààL2(X_n) \in L^2, centr√©es, deux √† deux non corr√©l√©es, et
> 
> 1n‚àëVar(Xk)‚ü∂0,\frac{1}{n} \sum \text{Var}(X_k) \longrightarrow 0,
> 
> alors :
> 
> Snn‚ÜíL20\frac{S_n}{n} \xrightarrow{L^2} 0
---
### üéØ ASTUCES
- La m√©thode ici repose uniquement sur la **variance** de la somme et l‚Äô**in√©galit√© de Bienaym√©** (utilis√©e implicitement via le calcul d‚Äôesp√©rance quadratique).
- Cela montre qu‚Äôun comportement "moyen" dispara√Æt si la **variance moyenne** tend vers z√©ro :
    
    ‚Üí **Pas besoin d‚Äôind√©pendance**, seulement **non-corr√©lation**.
    
- Dans un contexte de partiel, toujours v√©rifier :
    - **Centrage** : E[Xn]=0\mathbb{E}[X_n] = 0,
    - **Var finie** : Xn‚ààL2X_n \in L^2,
    - **(Non)-corr√©lation** ou **ind√©pendance**.
---
### üìò **Exercice 14 ‚Äî Vecteurs gaussiens et transformation al√©atoire**
![[exercice14f2.pdf]]
### Contexte
Soient :
- X‚àºN(0,1)X \sim \mathcal{N}(0,1) (loi normale centr√©e r√©duite),
- Œµ‚àà{‚àí1,1}\varepsilon \in \{-1, 1\} telle que P(Œµ=‚àí1)=P(Œµ=1)=12\mathbb{P}(\varepsilon = -1) = \mathbb{P}(\varepsilon = 1) = \frac{1}{2},
- Œµ\varepsilon et XX ind√©pendants,
Et on d√©finit :
Y:=ŒµXY := \varepsilon X
---
### ‚úÖ R√©solution d√©taill√©e
### **1. Montrer que Y‚àºN(0,1)Y \sim \mathcal{N}(0,1)**
On utilise la **fonction de r√©partition** :
FY(x)=P(ŒµX‚â§x)=12P(‚àíX‚â§x)+12P(X‚â§x)=12P(X‚â•‚àíx)+12P(X‚â§x)F_Y(x) = \mathbb{P}(\varepsilon X \leq x)  
= \frac{1}{2} \mathbb{P}(-X \leq x) + \frac{1}{2} \mathbb{P}(X \leq x)  
= \frac{1}{2} \mathbb{P}(X \geq -x) + \frac{1}{2} \mathbb{P}(X \leq x)
La loi normale centr√©e est **sym√©trique**, donc P(X‚â§x)=P(X‚â•‚àíx)\mathbb{P}(X \leq x) = \mathbb{P}(X \geq -x).
Ainsi :
FY(x)=P(X‚â§x)‚áíY‚àºN(0,1)F_Y(x) = \mathbb{P}(X \leq x)  
\Rightarrow Y \sim \mathcal{N}(0,1)
---
### **2. Calculer Cov(X,Y)\text{Cov}(X, Y)**
Cov(X,Y)=E[XY]‚àíE[X]E[Y]=E[ŒµX2]\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = \mathbb{E}[\varepsilon X^2]
Par ind√©pendance :
E[ŒµX2]=E[Œµ]‚ãÖE[X2]=0‚ãÖ1=0‚áíCov(X,Y)=0\mathbb{E}[\varepsilon X^2] = \mathbb{E}[\varepsilon] \cdot \mathbb{E}[X^2] = 0 \cdot 1 = 0  
\Rightarrow \text{Cov}(X, Y) = 0
---
### **3. D√©terminer la loi de Z:=X+YZ := X + Y**
Deux cas :
- Si Œµ=1\varepsilon = 1 : Z=2X‚àºN(0,4)Z = 2X \sim \mathcal{N}(0, 4)
- Si Œµ=‚àí1\varepsilon = -1 : Z=0Z = 0
Donc ZZ prend la valeur 0 avec probabilit√© 12\frac{1}{2}, et suit une loi N(0,4)\mathcal{N}(0,4) avec probabilit√© 12\frac{1}{2}.
Donc la fonction de r√©partition est :
FZ(z)={12F2X(z)si¬†z<012+12F2X(z)si¬†z‚â•0F_Z(z) =  
\begin{cases}  
\frac{1}{2} F_{2X}(z) & \text{si } z < 0 \\  
\frac{1}{2} + \frac{1}{2} F_{2X}(z) & \text{si } z \geq 0  
\end{cases}
Cette fonction **pr√©sente un saut en 0**, donc **ZZ n‚Äôa pas de densit√©**.
---
### **4. (X,Y)T(X, Y)^T n‚Äôest pas un vecteur gaussien**
Car **X+YX + Y** n‚Äôa pas une loi normale ‚Üí contradiction avec la d√©finition d‚Äôun **vecteur gaussien**, qui impose que **toute combinaison lin√©aire** suive une loi normale.
---
## üß† FICHE DE SYNTH√àSE ‚Äì POUR LE PARTIEL
### üß© D√©finitions cl√©s
- **Vecteur gaussien** : (X1,...,Xn)T(X_1, ..., X_n)^T est gaussien si **toute combinaison lin√©aire** a1X1+‚ãØ+anXna_1 X_1 + \dots + a_n X_n est une variable normale.
- **Loi sym√©trique** : X‚àºN(0,1)‚áíX‚àíXX \sim \mathcal{N}(0,1) \Rightarrow X \overset{d}{=} -X
- **Covariance** :
    
    Cov(X,Y)=E[XY]‚àíE[X]E[Y]\text{Cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y]
    
---
### üí° Propri√©t√©s utiles
- Si Œµ‚àà{‚àí1,1}\varepsilon \in \{-1,1\} ind√©pendante de X‚àºN(0,1)X \sim \mathcal{N}(0,1), alors ŒµX‚àºN(0,1)\varepsilon X \sim \mathcal{N}(0,1)
- Cov(X,ŒµX)=E[Œµ]‚ãÖE[X2]\text{Cov}(X, \varepsilon X) = \mathbb{E}[\varepsilon] \cdot \mathbb{E}[X^2], donc = 0 si E[Œµ]=0\mathbb{E}[\varepsilon] = 0
- Si une **combinaison lin√©aire** n‚Äôest pas normale, le vecteur n‚Äôest **pas gaussien**
---
### üéØ ASTUCES
- V√©rifie bien si **toute combinaison lin√©aire** est normale pour justifier qu‚Äôun vecteur est gaussien.
- Attention : **covariance nulle ‚â† ind√©pendance**, sauf cas normal.
- Si tu trouves une variable qui prend **une valeur fixe avec proba >0> 0** + densit√© ailleurs ‚Üí ce n‚Äôest **ni discret ni continu**.
---
### üìò **Exercice 15 ‚Äî Vecteur gaussien et ind√©pendance**
### Enonc√©
Soient XX et YY deux variables al√©atoires **ind√©pendantes**, toutes deux suivant une loi normale centr√©e r√©duite :
X,Y‚àºN(0,1)X, Y \sim \mathcal{N}(0,1)
On d√©finit :
- U=X+YU = X + Y
- V=X‚àíYV = X - Y
On demande :
1. Montrer que (U,V)T(U, V)^T est un **vecteur gaussien**
2. Montrer que UU et VV sont **ind√©pendantes**
---
## ‚úÖ **R√©solution**
### 1. Montrer que (U,V)T(U, V)^T est un vecteur gaussien
Rappel : un vecteur al√©atoire est **gaussien** si **toute combinaison lin√©aire** des composantes suit une **loi normale**.
On a :
U=X+Y,V=X‚àíYU = X + Y, \quad V = X - Y
Or XX et YY sont **ind√©pendantes** et **normales** ‚áí le vecteur (X,Y)T(X, Y)^T est gaussien.
Et toute **application lin√©aire** d‚Äôun vecteur gaussien reste gaussienne.
Donc :
(U,V)T=A‚ãÖ(X,Y)T,ouÀã¬†A=(111‚àí1)(U, V)^T = A \cdot (X, Y)^T, \quad \text{o√π } A =  
\begin{pmatrix}  
1 & 1 \\  
1 & -1  
\end{pmatrix}
üîπ **Conclusion** :
(U,V)T¬†est¬†un¬†vecteur¬†gaussien(U, V)^T \text{ est un vecteur gaussien}
---
### 2. Montrer que UU et VV sont ind√©pendantes
Pour deux variables **jointement normales**, **l‚Äôind√©pendance √©quivaut √† la non-corr√©lation**.
On calcule donc Cov(U,V)\text{Cov}(U, V).
### a) Esp√©rances :
E[U]=E[X+Y]=0,E[V]=E[X‚àíY]=0\mathbb{E}[U] = \mathbb{E}[X + Y] = 0, \quad \mathbb{E}[V] = \mathbb{E}[X - Y] = 0
### b) Covariance :
Cov(U,V)=E[UV]=E[(X+Y)(X‚àíY)]=E[X2‚àíY2]\text{Cov}(U, V) = \mathbb{E}[UV] = \mathbb{E}[(X+Y)(X-Y)] = \mathbb{E}[X^2 - Y^2]
Comme X‚àºN(0,1)X \sim \mathcal{N}(0,1) et Y‚àºN(0,1)Y \sim \mathcal{N}(0,1) :
E[X2]=E[Y2]=1‚áíCov(U,V)=1‚àí1=0\mathbb{E}[X^2] = \mathbb{E}[Y^2] = 1  
\Rightarrow \text{Cov}(U, V) = 1 - 1 = 0
üîπ UU et VV sont **non corr√©l√©es**, et comme elles sont **jointement normales**, cela implique qu'elles sont **ind√©pendantes**.
---
## üß† FICHE DE SYNTH√àSE ‚Äì √Ä RETENIR POUR LE PARTIEL
### üß© Notions cl√©s
- **Vecteur gaussien** : toute combinaison lin√©aire de ses composantes suit une loi normale.
- **Application lin√©aire d‚Äôun vecteur gaussien** ‚Üí donne encore un vecteur gaussien.
- **Ind√©pendance pour des lois normales** :
    - Deux variables normales **jointement normales** sont **ind√©pendantes si et seulement si elles sont non corr√©l√©es**.
---
### üí° Propri√©t√©s utiles
Soit X,Y‚àºN(0,1)X, Y \sim \mathcal{N}(0,1) ind√©pendantes :
- U=X+Y‚àºN(0,2)U = X + Y \sim \mathcal{N}(0,2)
- V=X‚àíY‚àºN(0,2)V = X - Y \sim \mathcal{N}(0,2)
- Cov(U,V)=0‚áíU‚ä•V\text{Cov}(U, V) = 0 \Rightarrow U \perp V
---
### üéØ ASTUCES
- Si on te demande si un **vecteur est gaussien**, v√©rifie si c‚Äôest une **transformation lin√©aire** d‚Äôun vecteur de normales.
- Si tu montres que deux variables normales sont **non corr√©l√©es**, et qu‚Äôelles viennent d‚Äôun **vecteur gaussien**, alors elles sont **automatiquement ind√©pendantes**.
- √Ä l‚Äôoral ou √† l‚Äô√©crit, **insiste bien sur l‚Äôimplication "normales + non corr√©l√©es ‚áí ind√©pendantes"** (ce n‚Äôest pas vrai en g√©n√©ral).
---
Souhaites-tu qu‚Äôon fasse l‚Äôexercice 16 aussi ? C‚Äôest la suite logique.