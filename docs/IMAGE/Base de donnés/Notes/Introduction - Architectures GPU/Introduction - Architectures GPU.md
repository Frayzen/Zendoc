---
title: Introduction - Architectures GPU
Mati√®re:
  - "[[Impl√©mentation Rapide GPGPU]]"
Type: Cours
Date du cours: 2025-03-19
Supports:
  - "[[j1-part1-history.slides.pdf]]"
  - "[[j1-part2-forms_of_parallelism.slides.pdf]]"
  - "[[j1-part3-gpu_vs_cpu_parallelism.slides.pdf]]"
  - "[[j1-part4-gpu_programming_model.slides.pdf]]"
---
# Histoire des GPU
  
## Introduction
Les GPU (Graphics Processing Units) ont √©volu√© des processeurs graphiques des ann√©es 1970 √† des unit√©s de calcul parall√®le puissantes, utilis√©es dans le gaming, la simulation scientifique, l'apprentissage profond, etc.
  
---
  
## Pourquoi Utiliser les GPU ?
  
![[image 91.png|image 91.png]]

### 1. **D√©veloppement Mobile : Optimisation de l'√ânergie**
- Les GPU sont plus efficaces que les CPU pour les t√¢ches graphiques et parall√®les.
- R√©duit la consommation d'√©nergie, crucial pour l'autonomie des smartphones.
- **Exemple** : Jeux mobiles o√π le GPU g√®re les calculs graphiques, laissant le CPU inactif.
![[image 1 24.png|image 1 24.png]]

### 2. **Big Data : Traitement de Donn√©es Massives**
- Les GPU traitent des milliers de threads simultan√©ment, id√©al pour les grands ensembles de donn√©es.
- **Exemple** : Algorithmes de machine learning (TensorFlow, PyTorch) optimis√©s pour GPU.
### 3. **Calcul en Temps R√©el : R√©ponse Rapide**
- Les GPU doivent fournir des r√©sultats dans un temps limit√©.
- **Exemple** : Jeux vid√©o (calcul de frames), conduite autonome (traitement des capteurs en temps r√©el).
  
---
## Les Origines des GPU (Ann√©es 1970)
  
### Ikonas RDS-3000
- Premier GPU pour la synth√®se d'images.
- D√©velopp√© par Ikonas Graphics Systems (N. England & M. Whitton).
- **Tim Van Hook** : Microcode pour le ray tracing (SIGGRAPH '86).
  
---
## L'√âmergence des GPGPU (1999-2001)
  
### Premiers GPU Programmables
- **Vertex Shaders** : Transformations de vertex en virgule flottante 32 bits.
- **Texturage Configurable** : Combinateurs de registres.
![[ikonas.mp4]]

### Applications Pr√©coces
- **Hoff (1999)** : Diagrammes de Voronoi sur NVIDIA TNT2.
- **Larsen & McAllister (2001)** : Multiplication de matrices (8 bits).
- **Rumpf & Strzodka (2001)** : R√©solution d'EDP (diffusion, segmentation d'images).
  
### GeForce 3 (2001)
- Introduit les **shaders programmables**.
- **Vertex Shaders** : Transformations de vertex.
- **Pixel Shaders** : Effets de texture et √©clairage r√©alistes.
- **Impact** : Base pour les GPU programmables modernes.
![[738-front.jpg]]

  
---
## L'√àre de la Virgule Flottante (2003)
  
### GeForce FX
- Premier GPU avec calcul en **virgule flottante**.
- **Ray Tracing** : Lanc√© par Purcell (2002).
- **Radiosit√©** : Calculs d'√©clairage global (Carr et al., 2003).
- **Simulations Physiques** : Fluides, nuages, tissus, cristaux de glace.
- **FFT** : Transform√©e de Fourier rapide (Moreland et Angel, 2003).
- **Brook for GPUs** : Langage de haut niveau (Buck et al., 2004).
![[s-l1200.jpg]]

  
---
## L'Explosion du GPGPU (2006)
### Architecture NVIDIA G80
- Con√ßue pour le calcul parall√®le.
- **Mode de Calcul D√©di√©** : Threads ind√©pendants.
- **Architecture M√©moire G√©n√©rale** : Byte-adressable.
### CUDA
- Plateforme de programmation pour C/C++.
- Rend les GPU accessibles pour le calcul scientifique et industriel.
---
## Les Ann√©es 2010 : Calcul Haute Performance (HPC)
### Recherche Scientifique
- **Simulation du Virus HIV** : Premi√®re simulation atomique compl√®te avec 3 000 GPU Tesla.
- **Centres de Donn√©es** : Adoption dans le p√©trole, la finance, l'√©ducation.
### Apprentissage Profond
- Les GPU acc√©l√®rent l'entra√Ænement des r√©seaux de neurones.
- **Impact** : Rendu obsol√®tes certaines m√©thodes traditionnelles de traitement d'images.
  
---
## Syst√®mes Embarqu√©s et Temps R√©el
- **Contraintes** : Temps r√©el et consommation d'√©nergie.
- **Exemple** : √âtiquetage de sc√®nes avec apprentissage hi√©rarchique (Farabet et al., 2013).
![[image 2 24.png|image 2 24.png]]

  
---
## Conclusion
Les GPU ont r√©volutionn√© le calcul parall√®le, passant du rendu graphique √† des applications scientifiques et industrielles. Leur √©volution continue de repousser les limites de la performance et de l'efficacit√© √©nerg√©tique.
---
  
  
# La Programmation Parall√®le
  
  
## **Fifty Shades of Parallelism**
  
### üèÉ‚Äç‚ôÇÔ∏è Comment ex√©cuter plus rapidement un programme ?
  
Il existe plusieurs mani√®res d‚Äôacc√©l√©rer un programme :
1. **Faire moins de travail** (optimisation des algorithmes et suppression des calculs inutiles).
2. **Faire mieux un travail donn√©** (en optimisant les structures de donn√©es et en ciblant les parties les plus co√ªteuses en temps).
3. **Effectuer plusieurs t√¢ches en parall√®le** (exploiter le parall√©lisme).
4. **Distribuer le travail sur plusieurs unit√©s de calcul** (utilisation efficace des ressources mat√©rielles).
  
‚ö° **Enjeux du parall√©lisme :**
Avec la fin de la **loi de Moore**, les processeurs n‚Äôaugmentent plus aussi vite en puissance, d‚Äôo√π la n√©cessit√© d‚Äôexploiter des architectures parall√®les.
Les processeurs deviennent intelligents :
- Ex√©cution hors service / renommage dynamique des registres
- Ex√©cution sp√©culative avec pr√©diction de branchement
Les processeurs deviennent super-scalaires :
- pendant que la fr√©quence d'horloge du processeur est limit√©e‚Ä¶
- ‚Ä¶ la quantit√© de donn√©es √† traiter a explos√©¬†!
Il nous faut une autre fa√ßon de concevoir la ¬´¬†vitesse¬†¬ª.
![[image 3 21.png|image 3 21.png]]

![[image 4 20.png|image 4 20.png]]

  
---
## **Exemple : La cha√Æne de montage des burgers üçî**
  
Un bon exemple de parall√©lisme est une cha√Æne de production de sandwiches :
- **Optimisation de la latence** : minimiser le temps n√©cessaire pour produire **un seul** sandwich.
- **Optimisation du d√©bit** : maximiser le nombre de sandwiches produits par unit√© de temps.
### üìå **Deux types d'optimisation :**
1. **Latence** (MIMD collaboratif) : plusieurs personnes travaillent **ensemble** sur le m√™me sandwich.
    
    ![[image 5 20.png|image 5 20.png]]

    
2. **D√©bit** (MIMD horizontal) : chaque personne fait son propre sandwich.
    
    ![[image 6 19.png|image 6 19.png]]

    
üîé **Illustration avec diff√©rentes formes de parall√©lisme :**
- **Pipeline (MIMD vertical)** : chaque travailleur ex√©cute une √©tape sp√©cifique de la pr√©paration en s√©quence.
    
    ![[image 7 18.png|image 7 18.png]]

    
- **SIMD (DLP - Data Level Parallelism)** : un travailleur avec plusieurs bras fait plusieurs sandwiches simultan√©ment.
    
    ![[image 8 16.png|image 8 16.png]]

    

> [!important] Temps pour faire 1 sandwitch : $\frac{s}{4}$ (400% de speed-up)
  
---
## **√âvolution des processeurs : Vers plus de c≈ìurs**
  
L'√©volution des processeurs suit une tendance :
- **Baisse de la fr√©quence d‚Äôhorloge**.
- **Augmentation du nombre de c≈ìurs**.
- **Utilisation d‚Äôinstructions vectorielles (SIMD)**.
**Exemple d‚Äô√©volution des processeurs Intel Xeon** :
![[image 9 16.png|image 9 16.png]]

![[image 10 15.png|image 10 15.png]]

**Tendance cl√©** :
- **Performance unitaire en baisse** (les c≈ìurs sont moins rapides individuellement).
- **Performance globale en hausse** (gr√¢ce √† l‚Äôaugmentation du nombre de c≈ìurs).
  
---
  
## **Comparaison CPU vs GPU**
  
![[image 11 14.png|image 11 14.png]]

- **Les CPU** sont optimis√©s pour la **latence** (ex√©cution rapide de t√¢ches s√©quentielles).
- **Les GPU** sont optimis√©s pour le **d√©bit** (ex√©cution massive de t√¢ches parall√®les).
### **Exemple d‚Äôapplication :**
Dans le calcul scientifique, on combine souvent **CPU et GPU** pour exploiter les avantages des deux architectures.
  
---
  
## **Vers des architectures h√©t√©rog√®nes (CPU + GPU)**
  
Un bon √©quilibre entre CPU et GPU est n√©cessaire pour maximiser les performances. On utilise :
1. **Le CPU pour les t√¢ches s√©quentielles** (optimis√© pour la latence).
2. **Le GPU pour les t√¢ches massivement parall√®les** (optimis√© pour le d√©bit).
### **Loi d‚ÄôAmdahl** :
Le gain maximal obtenu avec un programme parall√®le d√©pend de la fraction **P** du code qui est parall√©lisable.
Le speed-up pour $N$ processeurs :
$S = \frac{t_\text{old}}{t_\text{new}} = \frac{1}{(1 - P) + \frac{P}{N}}$
- $(1-P)$ : Temps pour faire tourner la partie ==**s√©quentielle**==
- $P/N$ : Temps pour faire tourner la partie ==**parall√®le**==
Exemple avec $P = 80%$ (80% du code parall√©lisable) :
- **Si** $N ‚Üí ‚àû$ **(c≈ìurs infinis)**, le **speedup maximal** est **5x**.
- **Si** $N = 4$, le **speedup est limit√©** par la partie s√©quentielle restante.
![[image 12 14.png|image 12 14.png]]

### Optimis√© pour la Latence - Multi-coeur CPU
üëé Mauvaises performances sur les portions parall√®les
![[image 13 14.png|image 13 14.png]]

### Optimis√© pour le d√©bit - GPU
üëé Mauvaises performances sur les portions parall√®les
![[image 14 14.png|image 14 14.png]]

### H√©t√©rog√®ne - CPU + GPU
üëç Utilisez le bon outil pour chaque t√¢che.
üëç Permet une optimisation agressive de la latence ou du d√©bit.
![[image 15 14.png|image 15 14.png]]

### **Conclusion :**
Une approche hybride **CPU + GPU** permet d'optimiser √† la fois la latence et le d√©bit.
  
---
## **R√©sum√©**
  
- **Le parall√©lisme est essentiel** pour compenser les limites des CPU modernes.
- **Diff√©rentes formes de parall√©lisme existent** (MIMD, SIMD, pipeline, etc.).
- **Les CPU et GPU sont compl√©mentaires** : les CPU g√®rent bien les t√¢ches s√©quentielles, tandis que les GPU excellent dans le traitement parall√®le.
- **Les architectures h√©t√©rog√®nes (CPU + GPU) sont l‚Äôavenir** pour maximiser la performance.
  
---
  
  
# Architecture CPU vs GPU
  
## Introduction
Les GPU (Graphics Processing Units) et CPU (Central Processing Units) sont con√ßus pour des t√¢ches diff√©rentes, ce qui se refl√®te dans leurs architectures. Ce cours explore ces diff√©rences, en mettant l'accent sur le parall√©lisme massif des GPU et le concept cl√© des **warps**.
  
---
## Architectures CPU vs GPU : Philosophie de Conception
  
### CPU : Low-Latency
![[image 16 14.png|image 16 14.png]]

- **Optimis√© pour** : Ex√©cution rapide de t√¢ches s√©quentielles complexes
- **Caract√©ristiques cl√©s** :
    - Moins d'unit√©s de calcul (ALU) mais tr√®s performantes
    - Hi√©rarchie de m√©moire complexe (caches L1/L2/L3)
    - Logique de contr√¥le sophistiqu√©e :
        - Ex√©cution sp√©culative
        - Ex√©cution "out-of-order"
==**Exemple :**== ==Intel i7 (8 ALUs, 4 c≈ìurs physiques)==
### GPU : High-Throughput
![[image 17 14.png|image 17 14.png]]

- **Optimis√© pour** : Traitement parall√®le massif de donn√©es
- **Caract√©ristiques cl√©s** :
    - Centaines d'ALUs simples
    - M√©moire globale partag√©e (moins de caches)
    - Tol√©rance √† la latence m√©moire via le parall√©lisme
==**Exemple :**== ==NVIDIA Kepler (128 c≈ìurs CUDA, 16 multiprocesseurs)==

> [!important]
> 
> ### **Analogie** :
> 
> Le CPU est comme un chef √©toil√© pr√©parant des plats complexes un par un.
> 
> Le GPU est comme une cantine scolaire produisant des milliers de repas simples en parall√®le.
  
  
---
## Warps : Coeur du Mod√®le de Parall√©lisme des GPU
  
### D√©finition
Un **warp** est un groupe de 32 threads (sur NVIDIA) qui s'ex√©cutent **en lockstep** (m√™me instruction simultan√©ment sur diff√©rentes donn√©es).
### Fonctionnement
- **SIMT (Single Instruction Multiple Threads)** :
    - Tous les threads d'un warp ex√©cutent la m√™me instruction au m√™me cycle
    - Si divergence (branchements conditionnels), les chemins sont s√©rialis√©s ‚Üí "warp divergence"
- **Gestion des warps** :
    - Chaque SM (Streaming Multiprocessor) g√®re plusieurs warps simultan√©ment
    - Switch entre warps gratuit (√©tat sauvegard√© dans registres)
### Exemple Pratique
```C++
// Exemple CUDA : addition de vecteurs
__global__ void add(float *A, float *B, float *C) {
    int i = threadIdx.x;
    C[i] = A[i] + B[i]; // Ex√©cut√© en parall√®le par tous les threads du warp
}
```
_==‚Üí 32 additions ex√©cut√©es en parall√®le si warp complet==_
  
  
---
## Dissimulation de Latence : Approche CPU vs GPU
  
### CPU : Approche Traditionnelle
![[image 18 14.png|image 18 14.png]]

CPU core
- **Strat√©gie** :
    - Cache hierarchy r√©duisant les acc√®s m√©moire
    - Multithreading avec co√ªt de changement de contexte
- **Sch√©ma** :_Co√ªt √©lev√© du changement de contexte_
    
    ```Plain
    Thread 1: [Exec][Stall][Switch] ‚Üí Thread 2: [Exec][Stall][Switch]...
    ```
    
### GPU : Parall√©lisme Massif
![[image 19 14.png|image 19 14.png]]

GPU SMP (Streaming Multiprocessor)
- **Strat√©gie** :
    - Ex√©cuter d'autres warps pendant les acc√®s m√©moire
    - Pipeline profond masquant la latence
- **Sch√©ma** :_Aucun cycle processeur perdu_
    
    ```Plain
    Warp 1: [Query][Stall][Exec]
    Warp 2:       [Query][Stall][Exec]
    Warp 3:             [Query][Stall][Exec]
    ```
    
### **Loi de Little** :
Taux d'arriv√©e des clients¬†: D√©bit
Temps pass√© par le client¬†: Latence
La **concurrence** correspond au nombre d'√©l√©ments trait√©s simultan√©ment.
Nombre moyen de clients¬†:
$\text{Concurrency} = \text{Throughput} √ó \text{Latency}$
‚Üí Un GPU n√©cessite des milliers de threads actifs pour saturer sa bande passante.
![[image 20 13.png|image 20 13.png]]

![[image 21 12.png|image 21 12.png]]

  
  
---
## Formes de Parall√©lisme
  
### Parall√©lisme Vertical (Pipelining)
**Objectif** : Masquer la latence en superposant des √©tapes de traitement**M√©canisme** :
- D√©coupage des instructions en phases s√©quentielles (Fetch, Decode, Execute, Write-back)
**Sp√©cificit√© GPU** : Appliqu√© aux warps :
- Quand un warp attend des donn√©es, un autre prend sa place imm√©diatement
- **Co√ªt de changement** : 0 cycle (vs ~10-100 cycles sur CPU)
### Parall√©lisme Horizontal
**Objectif** : Augmenter le d√©bit par duplication d'unit√©s de calcul
**Exemple GPU** :
- 100s d'ALUs travaillant en parall√®le
- **Throughput** = Nombre d'op√©rations/cycle √ó Fr√©quence
  
  
---
## Niveaux de Parall√©lisme
  
### Instruction-Level Parallelism (ILP)
**D√©finition** : Ex√©cution simultan√©e d'instructions ind√©pendantes
**Exemple Slide 14** :
```Plain
1. add r3 ‚Üê r1, r2  \\
2. mul r0 ‚Üê r0, r1   ‚Üí Ex√©cution parall√®le possible
3. sub r1 ‚Üê r3, r0  /
```
### Comparaison Architecturale
|   |   |   |
|---|---|---|
|Type|CPU (Haswell)|GPU (Kepler)|
|**ILP** (Instruction)|8 instructions|2 instructions|
|**TLP** (Thread)|4c/8t (HyperThread)|16 SM √ó 64 warps|
|**DLP** (Data)|AVX (8√ó32b)|SIMT (32 threads/warp)|
### D√©tails Cl√©s :
- **ILP** : Ex√©cution superscalaire sur CPU vs limit√©e sur GPU
- **TLP** : Warps permettent des milliers de threads l√©gers
- **DLP** : SIMD (CPU) vs SIMT (GPU - plus flexible)
**Impl√©mentation**¬†:
- CPU : Ex√©cution superscalaire (Slide 18 : 8 instructions/cycle sur Haswell)
- GPU : Dual-issue limit√© (2 instructions/cycle sur Kepler)
### Thread-Level Parallelism (TLP)
**D√©finition**¬†: Ex√©cution concurrente de threads ind√©pendants
**Comparaison**¬†:
|   |   |   |
|---|---|---|
||**CPU**|**GPU**|
|**Unit√©**|Thread OS|Warp (32 threads)|
|**Co√ªt switch**|Lourd (sauvegarde contexte)|Nul (registres d√©di√©s)|
|**Slide 15**¬†: Illustration avec 2 threads CPU vs warps GPU|||
### Data-Level Parallelism (DLP)
**D√©finition**¬†: M√™me op√©ration appliqu√©e √† plusieurs donn√©es
**Mod√®les**¬†:
- **SIMD**¬†(CPU) : Op√©rations vectorielles (Slide 16 : AVX-256 = 8 floats)
    
    cpp
    
    Copy
    
    ```Plain
    _mm256_add_ps(a, b); // Addition sur 8 valeurs
    ```
    
- **SIMT**¬†(GPU) : Warps ex√©cutant la m√™me instruction (Slide 19 : 32 threads/warp)
    
    ```C
    kernel<<<...>>>(input); // Ex√©cut√© sur 1000s de threads
    ```
    
---
## Extraction du Parall√©lisme
|   |   |   |
|---|---|---|
|**Type**|**Horizontal (Volume)**|**Vertical (Pipelining)**|
|**ILP**|Superscalar (CPU)|Pipeline d'instructions|
|**TLP**|Multi-c≈ìurs / SMT|Interleaved multithreading|
|**DLP**|SIMD/SIMT|-|
**Key Insight**¬†: Les GPU maximisent TLP/DLP horizontaux, tandis que les CPU √©quilibrent les trois.
---
## 4. Comparaison Architecturale (Slides 18-19)
### Intel Haswell (CPU)
- **ILP**¬†: 8 instructions/cycle (8 ALUs)
- **TLP**¬†: 4 c≈ìurs physiques √ó 2 hyper-threads
- **DLP**¬†: AVX-256 (8 floats/cycle)
### NVIDIA Kepler (GPU)
- **ILP**¬†: 2 instructions/cycle (limit√©)
- **TLP**¬†: 16 SMs √ó 4 warps/SM = 64 warps actifs
- **DLP**¬†: 32 threads/warp √ó 128 cores = 4096 threads/SM
**Preuve Slide 19**¬†:
"GPU: focus on DLP, TLP horizontal and vertical"
---
## Synth√®se : Pourquoi les GPU Dominent en Throughput
1. **Sp√©cialisation**¬†:
    - 80% de la puce d√©di√©e aux ALUs (vs 20% sur CPU)
    - M√©moire optimis√©e pour bande passante (GDDR6)
2. **Efficacit√© des Warps**¬†:
    - Masquage de latence via 64 warps/SM pr√™ts √† s'ex√©cuter
    - Exemple Slide 11 : 65KB de concurrency n√©cessaire ‚Üí atteint par 2048 threads/SM
3. **Mod√®le de Programmation**¬†:
    - SIMT permet d'exprimer facilement le DLP
    - Exemple : Kernel CUDA appliqu√© √† 1M d'√©l√©ments
cuda
Copy
```Plain
__global__ void add(float *a, float *b) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    a[i] += b[i]; // Ex√©cut√© en parall√®le sur tous les threads
}
```
Copy
```Plain
---
### √Ä Int√©grer dans Notion :
1. **Diagrammes Cl√©s** :
   - Slide 16 : Illustration SIMD vs SIMT
   - Slide 19 : Tableau comparatif CPU/GPU
2. **Exemples Concrets** :
   ```python
   # Calcul de throughput th√©orique GPU
   alus_per_sm = 128
   clock_speed = 1e9 # 1 GHz
   throughput = alus_per_sm * clock_speed # 128 GFLOPs/SM
```
  
  
---
## Optimisation pour GPU : Bonnes Pratiques
  
1. **Maximiser l'occupation des warps** :
    - √âviter la divergence (if/else dans un warp)
    - Utiliser des warps complets (tailles de bloc multiples de 32)
2. **Acc√®s m√©moire coalesc√©s** :
    - Les threads d'un warp doivent acc√©der √† des adresses contigu√´s
3. **Masquer la latence** :
    - Avoir suffisamment de warps actifs par SM (typiquement 64 warps/SM)
  
---
## Conclusion
  
- **GPU** : Roi du throughput via warps et parall√©lisme massif
- **CPU** : Ma√Ætre de la latence pour t√¢ches complexes
- **Choix** : D√©pend de l'application (algorithmes parall√©lisables vs s√©quentiels)
**Perspective** : Les architectures modernes (e.g., Grace Hopper) combinent les deux approches avec des CPU et GPU int√©gr√©s.
  
---
  
  
# Mod√®le de m√©moire GPU
  
## Introduction
Ce cours plonge dans le mod√®le de programmation des GPU, r√©v√©lant comment les concepts logiciels (threads, blocs, grids) se mappent sur l'architecture mat√©rielle (SM, warps, c≈ìurs CUDA). Nous explorerons √©galement le mod√®le m√©moire hi√©rarchique et les implications du paradigme SIMT.
---
## 1. Mod√®le de Programmation Hi√©rarchique
### 1.1 Les Trois Niveaux d'Abstraction
|   |   |   |
|---|---|---|
|Niveau|Description|Analogie|
|**Thread**|Unit√© de base d'ex√©cution|Ouvrier sur une cha√Æne|
|**Block**|Groupe de threads coop√©rants (‚â§ 1024)|√âquipe dans un atelier|
|**Grid**|Ensemble de blocs ex√©cutant le m√™me kernel|Usine enti√®re|
**Exemple en CUDA** :
```C++
// Kernel 1D : N threads ind√©pendants
__global__ void vecAdd(float* A, float* B, float* C) {
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    C[i] = A[i] + B[i]; // Chaque thread traite un √©l√©ment
}
```
  
### 1.2 M√©moire Associ√©e
|   |   |   |
|---|---|---|
|Niveau|M√©moire|Port√©e|
|Thread|Registres, m√©moire locale|Priv√©e|
|Block|M√©moire partag√©e (SMEM)|Tous les threads du bloc|
|Grid|M√©moire globale|Tous les threads|
  
---
## 2. Mapping Mat√©riel : SMs et Warps
  
### 2.1 Streaming Multiprocessors (SMs)
- **R√¥le** : Ex√©cuter les blocs de threads
- **Exemple (GTX 980)** : 16 SMs ind√©pendants
- **Propri√©t√©s cl√©s** :
    - Un bloc est assign√© √† un SM pour toute sa dur√©e de vie
    - Plusieurs blocs peuvent coexister sur un SM (si ressources suffisantes)
### 2.2 Warps : L'Unit√© R√©elle d'Ex√©cution
- **D√©finition** : Groupe de 32 threads ex√©cut√©s en **lockstep** (SIMD)
- **M√©canisme** :
    - Chaque SM partitionne les threads en warps
    - 4 warps schedulers par SM (NVIDIA Kepler)
    - 2 instructions ind√©pendantes peuvent √™tre dispatch√©es par cycle (ILP)
**Exemple** :
- Bloc de 96 threads ‚Üí 3 warps (32+32+32)
- Bloc de 50 threads ‚Üí 2 warps (32+18, avec 14 threads inactifs)
  
---
## 3. Mod√®le d'Ex√©cution SIMT
  
### 3.1 Principe SIMT vs SIMD
|   |   |   |
|---|---|---|
|Caract√©ristique|SIMD (CPU)|SIMT (GPU)|
|Flexibilit√©|Instructions rigides|Branchements par thread|
|Divergence|Co√ªteuse|Masquage par warp|
|Programmation|Explicitement vectoriel|Apparente scalarisation|
### 3.2 Gestion de la Divergence
- **Sc√©nario id√©al** : Tous les threads d'un warp suivent le m√™me chemin
- **Divergence** :  
    ‚Üí Ex√©cution s√©rialis√©e des deux chemins avec masquage
    
    ```C++
    if (threadIdx.x % 2 == 0) {
        // Chemin A (50% des threads)
    } else {
        // Chemin B (50% des threads)
    }
    ```
    
**Co√ªt** :
- Meilleur cas : 1 instruction (pas de divergence)
- Pire cas : N instructions (N chemins divergents)
  
---
## 4. Hi√©rarchie M√©moire : Optimisation Critique
  
### 4.1 Comparaison CPU/GPU
|   |   |   |
|---|---|---|
|M√©moire|CPU (i7)|GPU (Kepler)|
|Cache L1/L2|~256 KB/c≈ìur|~16 KB/SM|
|Registres|16-32/c≈ìur|65K/SM|
|Bande passante|~50 GB/s|~300 GB/s|
### 4.2 Strat√©gies d'Optimisation
1. **Coalescence** : Acc√®s m√©moire contigus par warp
    - Bon : `A[threadIdx.x]`
    - Mauvais : `A[threadIdx.x * stride]`
2. **Utilisation de la SMEM** :
    - Acte comme un cache programmable
    - Exemple : R√©duction parall√®le
3. **√âviter la "bank conflict"** :
    - 32 banks en m√©moire partag√©e
    - Acc√®s simultan√©s √† la m√™me bank ‚Üí s√©rialisation
  
---
## 5. Consid√©rations √ânerg√©tiques
  
**Donn√©es cl√©s (NVIDIA GT200)** :
- 1 acc√®s DRAM = 80 nJ (~90 W)
- 1 op√©ration FMA = 3.6 nJ (~36 W)
‚Üí **Ratio** : 1 acc√®s m√©moire ‚âà 22 op√©rations flottantes
**Implications** :
- Optimiser d'abord les acc√®s m√©moire
- R√©utiliser les donn√©es via les registres/SMEM
- Pr√©f√©rer le calcul au rechargement
  
---
## 6. √âtude de Cas : Boucles sur GPU
  
### 6.1 Comportement des Warps
```Python
i = 0
while i < thread_id:  # thread_id varie par thread
    i += 1
```
‚Üí **Ex√©cution** :
- Le warp continue jusqu'√† ce que **tous** les threads terminent
- Cycles gaspill√©s pour les threads d√©j√† sortis
### 6.2 Optimisation
- **Pr√©f√©rer** les boucles avec bornes constantes
- **Utiliser** `#pragma unroll` quand possible
  
---
## Conclusion
  
- **Abstraction logicielle** : Threads/blocs/grids masquent la complexit√© mat√©rielle
- **R√©alit√© mat√©rielle** : Warps et SMs dictent les performances
- **R√®gles d'or** :
    1. Minimiser la divergence des warps
    2. Maximiser l'occupation des SMs
    3. Optimiser la localit√© m√©moire