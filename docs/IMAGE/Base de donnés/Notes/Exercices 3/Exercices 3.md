---
title: Exercices 3
MatiÃ¨re:
  - "[[ProbabilitÃ© et Statistiques]]"
Type: TP/TD
Supports:
  - "[[Feuille3.pdf]]"
---
Passons Ã  lâ€™**Exercice 1** de la **Feuille 3**, qui demande :

> DÃ©terminer un estimateur convergent et sans biais du paramÃ¨tre Î» pour la loi de Poisson.
---
### ğŸ§  **Contexte et rappel**
Une variable alÃ©atoire XX suivant une **loi de Poisson de paramÃ¨tre Î»>0\lambda > 0** a pour espÃ©rance et variance :
E(X)=Î»,Var(X)=Î»\mathbb{E}(X) = \lambda, \quad \text{Var}(X) = \lambda
On suppose que lâ€™on observe un **Ã©chantillon X1,â€¦,XnX_1, \dots, X_n** i.i.d. suivant une loi de Poisson(Î»).
---
### âœ… **MÃ©thode de rÃ©solution**
### ğŸ¯ Objectif :
Trouver un estimateur **sans biais** et **convergent** de Î»\lambda.
### ğŸ”§ MÃ©thode des moments (ou maximum de vraisemblance ici) :
On sait que :
E(X)=Î»â‡’XË‰n=1nâˆ‘i=1nXi\mathbb{E}(X) = \lambda \Rightarrow \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
est un estimateur naturel de Î»\lambda.
---
### ğŸ“Œ **Solution**
On pose :
Î»^n=XË‰n=1nâˆ‘i=1nXi\hat{\lambda}_n = \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
### ğŸ” VÃ©rifications :
- **Sans biais** :
    
    E(Î»^n)=E(1nâˆ‘i=1nXi)=1nâˆ‘i=1nE(Xi)=1nâ‹…nâ‹…Î»=Î»\mathbb{E}(\hat{\lambda}_n) = \mathbb{E}\left( \frac{1}{n} \sum_{i=1}^n X_i \right) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X_i) = \frac{1}{n} \cdot n \cdot \lambda = \lambda
    
- **Convergence** (loi faible des grands nombres) :
    
    Î»^nâ†’PÎ»\hat{\lambda}_n \xrightarrow{P} \lambda
    
Donc Î»^n\hat{\lambda}_n est **convergent** vers Î»\lambda **en probabilitÃ©** (voire en moyenne quadratique si on veut aller plus loin).
---
### ğŸ§© **SynthÃ¨se : PropriÃ©tÃ©s, astuces et mÃ©thodes**
### âœ… Estimateur obtenu :
Î»^n=XË‰n\hat{\lambda}_n = \bar{X}_n
### ğŸ“Œ PropriÃ©tÃ©s :
- **Sans biais** : E(Î»^n)=Î»\mathbb{E}(\hat{\lambda}_n) = \lambda
- **Consistant (convergent)** : Î»^nâ†’PÎ»\hat{\lambda}_n \xrightarrow{P} \lambda
- **Efficace** : dans la famille exponentielle (la Poisson en fait partie), la moyenne est lâ€™estimateur du maximum de vraisemblance et optimal.
### ğŸ› ï¸ Astuce :
- Quand lâ€™espÃ©rance dÃ©pend directement du paramÃ¨tre Ã  estimer, la moyenne empirique est souvent un excellent estimateur.
- Dans les lois du type exponentielle, binomiale, poissonâ€¦ les estimateurs du maximum de vraisemblance et des moments coÃ¯ncident souvent.
---
Souhaites-tu passer Ã  lâ€™exercice 2 ?
Passons Ã  lâ€™**Exercice 2** de la **Feuille 3** :

> DÃ©terminer un estimateur du paramÃ¨tre Î±\alpha pour la loi de Pareto par la mÃ©thode des moments. (cf. feuille 1)
---
### ğŸ§  **Contexte et rappel (Feuille 1, Exercice 13)**
La densitÃ© de la loi de Pareto (paramÃ¨tre Î±>0\alpha > 0) est donnÃ©e pour xâ‰¥1x \geq 1 par :
fX(x)=Î±xâˆ’Î±âˆ’1,fX(x)=0Â sinonf_X(x) = \alpha x^{-\alpha - 1}, \quad f_X(x) = 0 \text{ sinon}
### ğŸ§¾ EspÃ©rance (quand elle existe) :
E(X)={Î±Î±âˆ’1,siÂ Î±>1nonÂ deËŠfinie,sinon\mathbb{E}(X) = \begin{cases}  
\frac{\alpha}{\alpha - 1}, & \text{si } \alpha > 1 \\  
\text{non dÃ©finie}, & \text{sinon}  
\end{cases}
---
### âœ… **MÃ©thode de rÃ©solution (mÃ©thode des moments)**
Lâ€™idÃ©e est dâ€™Ã©galer lâ€™espÃ©rance thÃ©orique Ã  la moyenne empirique. Soit un Ã©chantillon X1,â€¦,XnX_1, \dots, X_n i.i.d. suivant une loi de Pareto(Î±). On note :
XË‰n=1nâˆ‘i=1nXi\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
On pose :
XË‰n=E(X)=Î±Î±âˆ’1\bar{X}_n = \mathbb{E}(X) = \frac{\alpha}{\alpha - 1}
---
### ğŸ” RÃ©solution de lâ€™Ã©quation
On rÃ©sout :
XË‰n=Î±Î±âˆ’1\bar{X}_n = \frac{\alpha}{\alpha - 1}
On multiplie les deux membres par Î±âˆ’1\alpha - 1 :
XË‰n(Î±âˆ’1)=Î±â‡’XË‰nÎ±âˆ’XË‰n=Î±â‡’XË‰nÎ±âˆ’Î±=XË‰nâ‡’Î±(XË‰nâˆ’1)=XË‰nâ‡’Î±^n=XË‰nXË‰nâˆ’1\bar{X}_n (\alpha - 1) = \alpha  
\Rightarrow \bar{X}_n \alpha - \bar{X}_n = \alpha  
\Rightarrow \bar{X}_n \alpha - \alpha = \bar{X}_n  
\Rightarrow \alpha(\bar{X}_n - 1) = \bar{X}_n  
\Rightarrow \hat{\alpha}_n = \frac{\bar{X}_n}{\bar{X}_n - 1}
---
### ğŸ“Œ **Conclusion : Estimateur**
Lâ€™estimateur par la mÃ©thode des moments est :
Î±^n=XË‰nXË‰nâˆ’1(valideÂ siÂ XË‰n>1)\boxed{\hat{\alpha}_n = \frac{\bar{X}_n}{\bar{X}_n - 1}}  
\quad \text{(valide si } \bar{X}_n > 1 \text{)}
---
### ğŸ§© **SynthÃ¨se : PropriÃ©tÃ©s, astuces et mÃ©thodes**
### âœ… Estimateur obtenu :
Î±^n=XË‰nXË‰nâˆ’1\hat{\alpha}_n = \frac{\bar{X}_n}{\bar{X}_n - 1}
### ğŸ“Œ PropriÃ©tÃ©s :
- **Sans biais ?** : non, gÃ©nÃ©ralement biaisÃ© (car fonction non linÃ©aire de XË‰n\bar{X}_n), mais **consistant** (converge vers Î±\alpha).
- **Convergent** : oui, car XË‰nâ†’PE(X)\bar{X}_n \xrightarrow{P} \mathbb{E}(X) implique Î±^nâ†’PÎ±\hat{\alpha}_n \xrightarrow{P} \alpha.
### ğŸ› ï¸ Astuce :
- **Toujours identifier lâ€™espÃ©rance thÃ©orique en fonction du paramÃ¨tre**.
- Ensuite **rÃ©soudre lâ€™Ã©quation** pour le paramÃ¨tre : mÃ©thode directe et utile en pratique.
---
Souhaites-tu quâ€™on continue avec lâ€™**Exercice 3** ?
Passons Ã  lâ€™**Exercice 3** de la **Feuille 3** :

> DÃ©terminer un estimateur du paramÃ¨tre pp pour une loi gÃ©omÃ©trique.
---
### ğŸ§  **Contexte et rappel**
Une variable alÃ©atoire XX suivant une **loi gÃ©omÃ©trique** de paramÃ¨tre pâˆˆ(0,1]p \in (0, 1] (modÃ©lisant le nombre dâ€™Ã©checs avant le premier succÃ¨s) a pour loi :
P(X=k)=(1âˆ’p)kp,kâˆˆNP(X = k) = (1 - p)^k p, \quad k \in \mathbb{N}
### ğŸ“ EspÃ©rance :
E(X)=1âˆ’ppâ‡’1p=E(X)+1\mathbb{E}(X) = \frac{1 - p}{p}  
\quad \Rightarrow \quad \frac{1}{p} = \mathbb{E}(X) + 1
---
### âœ… **MÃ©thode des moments**
On note un Ã©chantillon X1,...,XnX_1, ..., X_n i.i.d. suivant une loi gÃ©omÃ©trique de paramÃ¨tre pp, et on note la moyenne empirique :
XË‰n=1nâˆ‘i=1nXi\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
On Ã©galise la moyenne empirique Ã  l'espÃ©rance thÃ©orique :
XË‰n=1âˆ’ppâ‡’p=1XË‰n+1\bar{X}_n = \frac{1 - p}{p}  
\Rightarrow p = \frac{1}{\bar{X}_n + 1}
---
### ğŸ“Œ **Conclusion : Estimateur**
Lâ€™estimateur du paramÃ¨tre pp est :
p^n=1XË‰n+1\boxed{\hat{p}_n = \frac{1}{\bar{X}_n + 1}}
---
### ğŸ§© **SynthÃ¨se : PropriÃ©tÃ©s, astuces et mÃ©thodes**
### âœ… Estimateur obtenu :
p^n=1XË‰n+1\hat{p}_n = \frac{1}{\bar{X}_n + 1}
### ğŸ“Œ PropriÃ©tÃ©s :
- **Sans biais ?** Non, en gÃ©nÃ©ral biaisÃ©.
- **Convergent** : oui, car XË‰nâ†’PE(X)â‡’p^nâ†’Pp\bar{X}_n \xrightarrow{P} \mathbb{E}(X) \Rightarrow \hat{p}_n \xrightarrow{P} p
### ğŸ› ï¸ Astuce :
- Toujours exprimer lâ€™espÃ©rance thÃ©orique en fonction du paramÃ¨tre recherchÃ©.
- Puis inverser pour isoler le paramÃ¨tre.
---
Souhaites-tu continuer avec lâ€™**Exercice 4** ?
Passons Ã  lâ€™**Exercice 4** de la **Feuille 3** :

> 1. DÃ©terminer les estimateurs donnÃ©s par la mÃ©thode des moments pour une loi normale N(m,Ïƒ2)\mathcal{N}(m, \sigma^2).
> 
> **2. Sont-ils convergents ? Pourquoi ?**
---
### ğŸ§  **Contexte : loi normale N(m,Ïƒ2)\mathcal{N}(m, \sigma^2)**
### ğŸ“ ParamÃ¨tres :
- EspÃ©rance : E(X)=m\mathbb{E}(X) = m
- Variance : Var(X)=Ïƒ2\text{Var}(X) = \sigma^2
- Deux paramÃ¨tres Ã  estimer : mm et Ïƒ2\sigma^2
---
### âœ… 1. **MÃ©thode des moments**
Soit X1,...,XnX_1, ..., X_n un Ã©chantillon i.i.d. de N(m,Ïƒ2)\mathcal{N}(m, \sigma^2).
### ğŸ¯ Estimation de mm :
m^n=XË‰n=1nâˆ‘i=1nXi\hat{m}_n = \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
### ğŸ¯ Estimation de Ïƒ2\sigma^2 :
On utilise :
E(X2)=Var(X)+(E(X))2=Ïƒ2+m2\mathbb{E}(X^2) = \text{Var}(X) + (\mathbb{E}(X))^2 = \sigma^2 + m^2
On estime donc E(X2)\mathbb{E}(X^2) par la moyenne empirique du carrÃ© :
X2â€¾n=1nâˆ‘i=1nXi2\overline{X^2}_n = \frac{1}{n} \sum_{i=1}^n X_i^2
Alors :
X2â€¾nâ‰ˆÏƒ2+m2etXË‰n2â‰ˆm2â‡’Ïƒ^n2=X2â€¾nâˆ’XË‰n2\overline{X^2}_n \approx \sigma^2 + m^2 \quad \text{et} \quad \bar{X}_n^2 \approx m^2  
\Rightarrow \hat{\sigma}^2_n = \overline{X^2}_n - \bar{X}_n^2
Ce qui est en fait la **variance empirique non corrigÃ©e**.
---
### ğŸ“Œ **RÃ©sultat : Estimateurs des moments**
m^n=XË‰netÏƒ^n2=1nâˆ‘i=1nXi2âˆ’(1nâˆ‘i=1nXi)2\boxed{  
\hat{m}_n = \bar{X}_n  
\quad \text{et} \quad  
\hat{\sigma}^2_n = \frac{1}{n} \sum_{i=1}^n X_i^2 - \left(\frac{1}{n} \sum_{i=1}^n X_i \right)^2  
}
---
### âœ… 2. **Sont-ils convergents ?**
### ğŸ”¹ m^n\hat{m}_n :
Câ€™est une moyenne â†’ **converge vers mm** (loi faible des grands nombres).
### ğŸ”¹ Ïƒ^n2\hat{\sigma}^2_n :
Câ€™est aussi une moyenne (de Xi2X_i^2), donc converge vers E(X2)âˆ’m2=Ïƒ2\mathbb{E}(X^2) - m^2 = \sigma^2.
âœ… **Donc les deux estimateurs sont convergents** (consistants).
---
### ğŸ§© **SynthÃ¨se : PropriÃ©tÃ©s, astuces et mÃ©thodes**
### âœ… Estimateurs obtenus :
- Moyenne : m^n=XË‰n\hat{m}_n = \bar{X}_n
- Variance : Ïƒ^n2=X2â€¾nâˆ’XË‰n2\hat{\sigma}^2_n = \overline{X^2}_n - \bar{X}_n^2
### ğŸ“Œ PropriÃ©tÃ©s :
- **Convergents** : oui
- **Sans biais** :
    - m^n\hat{m}_n : sans biais
    - Ïƒ^n2\hat{\sigma}^2_n : **biaisÃ©** (il existe une version corrigÃ©e avec 1nâˆ’1\frac{1}{n-1})
### ğŸ› ï¸ Astuce :
- MÃ©thode des moments = Ã©galer moments empiriques aux moments thÃ©oriques.
- Penser Ã  la **relation fondamentale** : Var(X)=E(X2)âˆ’(E(X))2\text{Var}(X) = \mathbb{E}(X^2) - (\mathbb{E}(X))^2
---
Souhaites-tu quâ€™on passe Ã  lâ€™**Exercice 5** ?
Passons Ã  lâ€™**Exercice 6** de la **Feuille 3** :

> Soit Xâˆ¼U[0;Î¸]X \sim \mathcal{U}[0; \theta].
> 
> 1. Quelle est la densitÃ© de la variable alÃ©atoire XX ?
> 2. Quelle est son espÃ©rance ?
> 3. En dÃ©duire un estimateur du paramÃ¨tre Î¸\theta par la mÃ©thode des moments.
---
### âœ… 1. **DensitÃ© de la variable XX**
Si Xâˆ¼U[0;Î¸]X \sim \mathcal{U}[0; \theta], alors la densitÃ© est donnÃ©e par :
fX(x)={1Î¸,siÂ 0â‰¤xâ‰¤Î¸0,sinonf_X(x) = \begin{cases}  
\frac{1}{\theta}, & \text{si } 0 \le x \le \theta \\  
0, & \text{sinon}  
\end{cases}
---
### âœ… 2. **EspÃ©rance de XX**
E(X)=âˆ«0Î¸xÎ¸â€‰dx=1Î¸â‹…[x22]0Î¸=Î¸2\mathbb{E}(X) = \int_0^\theta \frac{x}{\theta} \, dx = \frac{1}{\theta} \cdot \left[ \frac{x^2}{2} \right]_0^\theta = \frac{\theta}{2}
---
### âœ… 3. **Estimateur de Î¸\theta par la mÃ©thode des moments**
Soit X1,...,XnX_1, ..., X_n un Ã©chantillon i.i.d. suivant U[0;Î¸]\mathcal{U}[0; \theta].
On note :
XË‰n=1nâˆ‘i=1nXi\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
On Ã©galise :
XË‰nâ‰ˆE(X)=Î¸2â‡’Î¸^n=2XË‰n\bar{X}_n \approx \mathbb{E}(X) = \frac{\theta}{2}  
\Rightarrow \boxed{\hat{\theta}_n = 2 \bar{X}_n}
---
### ğŸ§© **SynthÃ¨se : PropriÃ©tÃ©s, astuces et mÃ©thodes**
### âœ… Estimateur obtenu :
Î¸^n=2XË‰n\hat{\theta}_n = 2 \bar{X}_n
### ğŸ“Œ PropriÃ©tÃ©s :
- **Sans biais ?** Non, biaisÃ© (car E[maxâ¡(Xi)]\mathbb{E}[\max(X_i)] serait sans biais pour Î¸).
- **Convergent** : Oui, car XË‰nâ†’PÎ¸2â‡’Î¸^nâ†’PÎ¸\bar{X}_n \xrightarrow{P} \frac{\theta}{2} \Rightarrow \hat{\theta}_n \xrightarrow{P} \theta
### ğŸ› ï¸ Astuce :
- Dans une loi uniforme U[0;Î¸]\mathcal{U}[0; \theta], lâ€™espÃ©rance est simple Ã  calculer : Î¸/2\theta/2
- Si on veut un estimateur sans biais, on peut aussi utiliser :
    
    Î¸^n(sansÂ biais)=n+1nâ‹…maxâ¡(X1,...,Xn)\hat{\theta}_n^{\text{(sans biais)}} = \frac{n+1}{n} \cdot \max(X_1, ..., X_n)
    
---
Souhaites-tu que lâ€™on termine avec lâ€™**Exercice 5** ou passer Ã  un autre thÃ¨me ?